## Low-density graph codes that are optimal for source/channel coding and binning


Martin J. Wainwright Emin Martinian
Dept. of Statistics, and Tilda Consulting, Inc.
Dept. of Electrical Engineering and Computer Sciences Arlington, MA
University of California, Berkeley emin@alum.mit.edu
wainwrig@{eecs,stat}.berkeley.edu

Technical Report 730,
Department of Statistics, UC Berkeley,
April 2007


Abstract


We describe and analyze the joint source/channel coding properties of a class of sparse
graphical codes based on compounding a low-density generator matrix (LDGM) code with a
low-density parity check (LDPC) code. Our first pair of theorems establish that there exist
codes from this ensemble, with all degrees remaining bounded independently of block length,
that are simultaneously optimal as both source and channel codes when encoding and decoding
are performed optimally. More precisely, in the context of lossy compression, we prove that
finite degree constructions can achieve any pair (R, D) on the rate-distortion curve of the binary
symmetric source. In the context of channel coding, we prove that finite degree codes can achieve
any pair (C, p) on the capacity-noise curve of the binary symmetric channel. Next, we show that
our compound construction has a nested structure that can be exploited to achieve the WynerZiv bound for source coding with side information (SCSI), as well as the Gelfand-Pinsker bound
for channel coding with side information (CCSI). Although the current results are based on
optimal encoding and decoding, the proposed graphical codes have sparse structure and high
girth that renders them well-suited to message-passing and other efficient decoding procedures.

Keywords: Graphical codes; low-density parity check code (LDPC); low-density generator matrix
code (LDGM); weight enumerator; source coding; channel coding; Wyner-Ziv problem; GelfandPinsker problem; coding with side information; information embedding; distributed source coding.


### 1 Introduction

Over the past decade, codes based on graphical constructions, including turbo codes [3] and low
density parity check (LDPC) codes [17], have proven extremely successful for channel coding prob
lems. The sparse graphical nature of these codes makes them very well-suited to decoding using

efficient message-passing algorithms, such as the sum-product and max-product algorithms. The

asymptotic behavior of iterative decoding on graphs with high girth is well-characterized by the

density evolution method [25, 39], which yields a useful design principle for choosing degree dis
tributions. Overall, suitably designed LDPC codes yield excellent practical performance under


iterative message-passing, frequently very close to Shannon limits [7].

1


-----



However, many other communication problems involve aspects of lossy source coding, either

alone or in conjunction with channel coding, the latter case corresponding to joint source-channel

coding problems. Well-known examples include lossy source coding with side information (one

variant corresponding to the Wyner-Ziv problem [45]), and channel coding with side information

(one variant being the Gelfand-Pinsker problem [19]). The information-theoretic schemes achieving

the optimal rates for coding with side information involve delicate combinations of source and

channel coding. For problems of this nature—in contrast to the case of pure channel coding—the

use of sparse graphical codes and message-passing algorithm is not nearly as well understood. With

this perspective in mind, the focus of this paper is the design and analysis sparse graphical codes

for lossy source coding, as well as joint source/channel coding problems. Our main contribution

is to exhibit classes of graphical codes, with all degrees remaining bounded independently of the

blocklength, that simultaneously achieve the information-theoretic bounds for both source and

channel coding under optimal encoding and decoding.

#### 1.1 Previous and ongoing work

A variety of code architectures have been suggested for lossy compression and related problems in

source/channel coding. One standard approach to lossy compression is via trellis-code quantization

(TCQ) [26]. The advantage of trellis constructions is that exact encoding and decoding can be

performed using the max-product or Viterbi algorithm [24], with complexity that grows linearly

in the trellis length but exponentially in the constraint length. Various researchers have exploited

trellis-based codes both for single-source and distributed compression [6, 23, 37, 46] as well as
information embedding problems [5, 15, 42]. One limitation of trellis-based approaches is the

fact that saturating rate-distortion bounds requires increasing the trellis constraint length [43],

which incurs exponential complexity (even for the max-product or sum-product message-passing

algorithms).

Other researchers have proposed and studied the use of low-density parity check (LDPC) codes

and turbo codes, which have proven extremely successful for channel coding, in application to

various types of compression problems. These techniques have proven particularly successful for

lossless distributed compression, often known as the Slepian-Wolf problem [18, 40]. An attractive

feature is that the source encoding step can be transformed to an equivalent noisy channel de
coding problem, so that known constructions and iterative algorithms can be leveraged. For lossy

compression, other work [31] shows that it is possible to approach the binary rate-distortion bound

using LDPC-like codes, albeit with degrees that grow logarithmically with the blocklength.

A parallel line of work has studied the use of low-density generator matrix (LDGM) codes, which

correspond to the duals of LDPC codes, for lossy compression problems [30, 44, 9, 35, 34]. Focusing

on binary erasure quantization (a special compression problem dual to binary erasure channel

coding), Martinian and Yedidia [30] proved that LDGM codes combined with modified message
2


-----



passing can saturate the associated rate-distortion bound. Various researchers have used techniques

from statistical physics, including the cavity method and replica methods, to provide non-rigorous

analyses of LDGM performance for lossy compression of binary sources [8, 9, 35, 34]. In the limit

of zero-distortion, this analysis has been made rigorous in a sequence of papers [12, 32, 10, 14].

Moreover, our own recent work [28, 27] provides rigorous upper bounds on the effective rate
distortion function of various classes of LDGM codes. In terms of practical algorithms for lossy

binary compression, researchers have explored variants of the sum-product algorithm [34] or survey

propagation algorithms [8, 44] for quantizing binary sources.

#### 1.2 Our contributions

Classical random coding arguments [11] show that random binary linear codes will achieve both

channel capacity and rate-distortion bounds. The challenge addressed in this paper is the design

and analysis of codes with bounded graphical complexity, meaning that all degrees in a factor graph

representation of the code remain bounded independently of blocklength. Such sparsity is critical

if there is any hope to leverage efficient message-passing algorithms for encoding and decoding.

With this context, the primary contribution of this paper is the analysis of sparse graphical code

ensembles in which a low-density generator matrix (LDGM) code is compounded with a low-density

parity check (LDPC) code (see Fig. 2 for an illustration). Related compound constructions have

been considered in previous work, but focusing exclusively on channel coding [16, 36, 41]. In

contrast, this paper focuses on communication problems in which source coding plays an essential

role, including lossy compression itself as well as joint source/channel coding problems. Indeed, the
source coding analysis of the compound construction requires techniques fundamentally different

from those used in channel coding analysis. We also note that the compound code illustrated

in Fig. 2 can be applied to more general memoryless channels and sources; however, so as to bring

the primary contribution into sharp focus, this paper focuses exclusively on binary sources and/or

binary symmetric channels.

More specifically, our first pair of theorems establish that for any rate R ∈ (0, 1), there exist

codes from compound LDGM/LDPC ensembles with all degrees remaining bounded independently

of the blocklength that achieve both the channel capacity and the rate-distortion bound. To the

best of our knowledge, this is the first demonstration of code families with bounded graphical

complexity that are simultaneously optimal for both source and channel coding. Building on

these results, we demonstrate that codes from our ensemble have a naturally “nested” structure,

in which good channel codes can be partitioned into a collection of good source codes, and vice

versa. By exploiting this nested structure, we prove that codes from our ensembles can achieve

the information-theoretic limits for the binary versions of both the problem of lossy source coding

with side information (SCSI, known as the Wyner-Ziv problem [45]), and channel coding with side

information (CCSI, known as the Gelfand-Pinsker [19] problem). Although these results are based

3


-----



on optimal encoding and decoding, a code drawn randomly from our ensembles will, with high

probability, have high girth and good expansion, and hence be well-suited to message-passing and

other efficient decoding procedures.

The remainder of this paper is organized as follows. Section 2 contains basic background

material and definitions for source and channel coding, and factor graph representations of binary

linear codes. In Section 3, we define the ensembles of compound codes that are the primary

focus of this paper, and state (without proof) our main results on their source and channel coding

optimality. In Section 4, we leverage these results to show that our compound codes can achieve

the information-theoretic limits for lossy source coding with side information (SCSI), and channel

coding with side information (CCSI). Sections 5 and 6 are devoted to proofs that codes from the

compound ensemble are optimal for lossy source coding (Section 5) and channel coding (Section 6)

respectively. We conclude the paper with a discussion in Section 7. Portions of this work have

previously appeared as conference papers [28, 29, 27].

### 2 Background

In this section, we provide relevant background material on source and channel coding, binary

linear codes, as well as factor graph representations of such codes.

#### 2.1 Source and channel coding

A binary linear code C of block length n consists of all binary strings x ∈{0, 1}[n] satisfying a

set of m < n equations in modulo two arithmetic. More precisely, given a parity check matrix

H ∈{0, 1}[m][×][n], the code is given by the null space

C : = {x ∈{0, 1}[n] | Hx = 0} . (1)

Assuming the parity check matrix H is full rank, the code C consists of 2[n][−][m] = 2[nR] codewords,

where R = 1 − [m]

n [is the code rate.]

Channel coding: In the channel coding problem, the transmitter chooses some codeword x ∈ C

and transmits it over a noisy channel, so that the receiver observes a noise-corrupted version Y .

The channel behavior is modeled by a conditional distribution P(y | x) that specifies, for each

transmitted sequence Y, a probability distribution over possible received sequences {Y = y}. In

many cases, the channel is memoryless, meaning that it acts on each bit of C in an independent

manner, so that the channel model decomposes as P(y | x) = [�]i[n]=1 [f][i][(][x][i][;][ y][i][) Here each function]

fi(xi; yi) = P(yi | xi) is simply the conditional probability of observing bit yi given that xi was

transmitted. As a simple example, in the binary symmetric channel (BSC), the channel flips each

4


-----



transmitted bit xi with probability p, so that P(yi | xi) = (1 − p) I[xi = yi] + p (1 − I[xi ̸= yi]),

where I(A) represents an indicator function of the event A. With this set-up, the goal of the re
ceiver is to solve the channel decoding problem: estimate the most likely transmitted codeword,

given by x : = arg max
� x∈C [P][(][y][ |][ x][). The Shannon capacity [11] of a channel specifies an upper bound]

on the rate R of any code for which transmission can be asymptotically error-free. Continuing

with our example of the BSC with flip probability p, the capacity is given by C = 1 − h(p), where

h(p) : = −p log2 p − (1 − p) log2(1 − p) is the binary entropy function.

Lossy source coding: In a lossy source coding problem, the encoder observes some source se
quence S ∈S, corresponding to a realization of some random vector with i.i.d. elements Si ∼ PS.

The idea is to compress the source by representing each source sequence S by some codeword

x ∈ C. As a particular example, one might be interested in compressing a symmetric Bernoulli

source, consisting of binary strings S ∈{0, 1}[n], with each element Si drawn in an independent
and identically distributed (i.i.d.) manner from a Bernoulli distribution with parameter p = [1]

2 [.]

One could achieve a given compression rate R = [m]

n [by mapping each source sequence to some]

codeword x ∈ C from a code containing 2[m] = 2[nR] elements, say indexed by the binary sequences

z ∈{0, 1}[m]. In order to assess the quality of the compression, we define a source decoding map

x �→ S[�](x), which associates a source reconstruction S(x) with each codeword x ∈ C. Given some

[�]

distortion metric d : S × S → R+, the source encoding problem is to find the codeword with min
imal distortion—namely, the optimal encoding x : = arg min
� x∈C [d][(][S][�][(][x][)][, S][). Classical rate-distortion]

theory [11] specifies the optimal trade-offs between the compression rate R and the best achievable

average distortion D = E[d(S, S[�] )], where the expectation is taken over the random source sequences

S. For instance, to follow up on the Bernoulli compression example, if we use the Hamming metric

d(S, S[�] ) = n[1] �ni=1 [|][S][�][i][ −] [S][i][|][ as the distortion measure, then the rate-distortion function takes the]

form R(D) = 1 − h(D), where h is the previously defined binary entropy function.

We now provide definitions of “good” source and channel codes that are useful for future reference.

Definition 1. (a) A code family is a good D-distortion binary symmetric source code if for any

ǫ > 0, there exists a code with rate R < 1 − h (D) + ǫ that achieves Hamming distortion less than

or equal to D.

(b) A code family is a good BSC(p)-noise channel code if for any ǫ > 0 there exists a code with

rate R > 1 − h (p) − ǫ with error probability less than ǫ.

#### 2.2 Factor graphs and graphical codes

Both the channel decoding and source encoding problems, if viewed naively, require searching over

an exponentially large codebook (since |C| = 2[nR] for a code of rate R). Therefore, any practically

5


-----



useful code must have special structure that facilitates decoding and encoding operations. The

success of a large subclass of modern codes in use today, especially low-density parity check (LDPC)

codes [17, 38], is based on the sparsity of their associated factor graphs.


x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 x11 x12
n

m


n


y1 y2 y3 y4 y5 y6 y7 y8 y9 y10 y11 y12

m


c1 c2 c3 c4 c5 c6 z1 z2 z3 z4 z5 z6 z7 z8 z9

(a) (b)

Figure 1. (a) Factor graph representation of a rate R = 0.5 low-density parity check (LDPC) code
with bit degree dv = 3 and check degree d[′]c [= 6. (b) Factor graph representation of a rate][ R][ = 0][.][75]
low-density generator matrix (LDGM) code with check degree dc = 3 and bit degree dv = 4.

Given a binary linear code C, specified by parity check matrix H, the code structure can be

captured by a bipartite graph, in which circular nodes (◦) represent the binary values xi (or columns

of H), and square nodes (■) represent the parity checks (or rows of H). For instance, Fig. 1(a)

shows the factor graph for a rate R = 2[1] [code in parity check form, with][ m][ = 6 checks acting on]

n = 12 bits. The edges in this graph correspond to 1’s in the parity check matrix, and reveal the

subset of bits on which each parity check acts. The parity check code in Fig. 1(a) is a regular

code with bit degree 3 and check degree 6. Such low-density constructions, meaning that both the

bit degrees and check degrees remain bounded independently of the block length n, are of most

practical use, since they can be efficiently represented and stored, and yield excellent performance

under message-passing decoding. In the context of a channel coding problem, the shaded circular
nodes at the top of the low-density parity check (LDPC) code in panel (a) represent the observed

variables yi received from the noisy channel.

Figure 1(b) shows a binary linear code represented in factor graph form by its generator matrix

G. In this dual representation, each codeword x ∈{0, 1}[n] is generated by taking the matrix-vector

product of the form Gz, where z ∈{0, 1}[m] is a sequence of information bits, and G ∈{0, 1}[n][×][m] is

the generator matrix. For the code shown in panel (b), the blocklength is n = 12, and information

sequences are of length m = 9, for an overall rate of R = m/n = 0.75 in this case. The degrees of

the check and variable nodes in the factor graph are dc = 3 and dv = 4 respectively, so that the

associated generator matrix G has dc = 3 ones in each row, and dv = 4 ones in each column. When

the generator matrix is sparse in this setting, then the resulting code is known as a low-density

generator matrix (LDGM) code.

6


-----



#### 2.3 Weight enumerating functions

For future reference, it is useful to define the weight enumerating function of a code. Given a binary

linear code of blocklength m, its codewords x have renormalized Hamming weights w : = [∥][x]m[∥][1] that

range in the interval [0, 1]. Accordingly, it is convenient to define a function Wm : [0, 1] → R+ that,

for each w ∈ [0, 1], counts the number of codewords of weight w:


Wm(w) : = �x ∈ C | w = � ∥x∥1
���� m


, (2)
������


where ⌈·⌉ denotes the ceiling function. Although it is typically difficult to compute the weight enu
merator itself, it is frequently possible to compute (or bound) the average weight enumerator, where

the expectation is taken over some random ensemble of codes. In particular, our analysis in the

sequel makes use of the average weight enumerator of a (dv, d[′]c[)-regular LDPC code (see Fig. 1(a)),]

defined as

1
Am(w; dv, d[′]c[)] : = (3)
m [log][ E][ [][W][m][(][w][)]][,]

where the expectation is taken over the ensemble of all regular (dv, d[′]c[)-LDPC codes. For such]

regular LDPC codes, this average weight enumerator has been extensively studied in previous

work [17, 22].

### 3 Optimality of bounded degree compound constructions

In this section, we describe the compound LDGM/LDPC construction that is the focus of this

paper, and describe our main results on their source and channel coding optimality.

#### 3.1 Compound construction

Our main focus is the construction illustrated in Fig. 2, obtained by compounding an LDGM code

(top two layers) with an LDPC code (bottom two layers). The code is defined by a factor graph

with three layers: at the top, a vector x ∈{0, 1}[n] of codeword bits is connected to a set of n parity

checks, which are in turn connected by a sparse generator matrix G to a vector y ∈{0, 1}[m] of

information bits in the middle layer. The information bits y are also codewords in an LDPC code,

defined by the parity check matrix H connecting the middle and bottom layers.

In more detail, considering first the LDGM component of the compound code, each codeword

x ∈{0, 1}[n] in the top layer is connected via the generator matrix G ∈{0, 1}[n][×][m] to an information

sequence y ∈{0, 1}[m] in the middle layer; more specifically, we have the algebraic relation x = Gy.

Note that this LDGM code has rate RG ≤ [m]n [. Second, turning to the LDPC component of the]

compound construction, its codewords correspond to a subset of information sequences y ∈{0, 1}[m]

7


-----



in the middle layer. In particular, any valid codeword y satisfies the parity check relation Hy = 0,

where H ∈{0, 1}[m][×][k] joins the middle and bottom layers of the construction. Overall, this defines

an LDPC code with rate RH = 1 − m[k] [, assuming that][ H][ has full row rank.]

The overall code C obtained by concatenating the LDGM and LDPC codes has blocklength n,

and rate R upper bounded by RGRH. In algebraic terms, the code C is defined as

C : = {x ∈{0, 1}[n] | x = Gy for some y ∈{0, 1}[m] such that Hy = 0}, (4)

where all operations are in modulo two arithmetic.


n


dc


G

m dv

H1 H2

k d[′]c

k1 k2

Figure 2. The compound LDGM/LDPC construction analyzed in this paper, consisting of a (n, m)
LDGM code over the middle and top layers, compounded with a (m, k) LDPC code over the middle
and bottom layers. Codewords x ∈{0, 1}[n] are placed on the top row of the construction, and are
associated with information bit sequences z ∈{0, 1}[m] in the middle layer. The LDGM code over the
top and middle layers is defined by a sparse generator matrix G ∈{0, 1}[n][×][m] with at most dc ones
per row. The bottom LDPC over the middle and bottom layers is represented by a sparse parity
check matrix H ∈{0, 1}[k][×][m] with dv ones per column, and d[′]c [ones per row.]

Our analysis in this paper will be performed over random ensembles of compound LDGM/LDPC

ensembles. In particular, for each degree triplet (dc, dv, d[′]c[), we focus on the following random]
ensemble:

(a) For each fixed integer dc ≥ 4, the random generator matrix G ∈{0, 1}[n][×][m] is specified as

follows: for each of the n rows, we choose dc positions with replacement, and put a 1 in each

of these positions. This procedure yields a random matrix with at most dc ones per row,

since it is possible (although of asymptotically negligible probability for any fixed dc) that

the same position is chosen more than once.

(b) For each fixed degree pair (dv, d[′]c[), the random LDPC matrix][ H][ ∈{][0][,][ 1][}][k][×][m][ is chosen uni-]

formly at random from the space of all matrices with exactly dv ones per column, and exactly

d[′]c [ones per row. This ensemble is a standard (][d][v][, d]c[′] [)-regular LDPC ensemble.]

8


-----



We note that our reason for choosing the check-regular LDGM ensemble specified in step (a) is not

that it need define a particularly good code, but rather that it is convenient for theoretical purposes.

Interestingly, our analysis shows that the bounded degree dc check-regular LDGM ensemble, even

though it is sub-optimal for both source and channel coding in isolation [28, 29], defines optimal

source and channel codes when combined with a bottom LDPC code.

#### 3.2 Main results

Our first main result is on the achievability of the Shannon rate-distortion bound using codes from

LDGM/LDPC compound construction with finite degrees (dc, dv, d[′]c[). In particular, we make the]

following claim:

Theorem 1. Given any pair (R, D) satisfying the Shannon bound, there is a set of finite degrees

(dc, dv, d[′]c[)][ and a code from the associated LDGM/LDPC ensemble with rate][ R][ that is a][ D][-good]

source code (see Definition 1).

In other work [28, 27], we showed that standard LDGM codes from the check-regular ensemble

cannot achieve the rate-distortion bound with finite degrees. As will be highlighted by the proof of

Theorem 1 in Section 5, the inclusion of the LDPC lower code in the compound construction plays

a vital role in the achievability of the Shannon rate-distortion curve.

Our second main result of this result is complementary in nature to Theorem 1, regarding the

achievability of the Shannon channel capacity using codes from LDGM/LDPC compound construc
tion with finite degrees (dc, dv, d[′]c[). In particular, we have:]

Theorem 2. For all rate-noise pairs (R, p) satisfying the Shannon channel coding bound R <

1 − h (p), there is a set of finite degrees (dc, dv, d[′]c[)][ and a code from the associated LDGM/LDPC]

ensemble with rate R that is a p-good channel code (see Definition 1).

To put this result into perspective, recall that the overall rate of this compound construction is

given by R = RGRH. Note that an LDGM code on its own (i.e., without the lower LDPC code)

is a special case of this construction with RH = 1. However, a standard LDGM of this variety is

not a good channel code, due to the large number of low-weight codewords. Essentially, the proof

of Theorem 2 (see Section 6) shows that using a non-trivial LDPC lower code (with RH < 1) can

eliminate these troublesome low-weight codewords.

### 4 Consequences for coding with side information

We now turn to consideration of the consequences of our two main results for problems of coding

with side information. It is well-known from previous work [47] that achieving the information
theoretic limits for these problems requires nested constructions, in which a collection of good source

9


-----



codes are nested inside a good channel code (or vice versa). Accordingly, we begin in Section 4.1

by describing how our compound construction naturally generates such nested ensembles. In Sec
tions 4.2 and 4.3 respectively, we discuss how the compound construction can be used to achieve

the information-theoretic optimum for binary source coding with side information (a version of the

Wyner-Ziv problem [45]), and binary information embedding (a version of “dirty paper coding”,

or the Gelfand-Pinsker problem [19]).

#### 4.1 Nested code structure

The structure of the compound LDGM/LDPC construction lends itself naturally to nested code

constructions. In particular, we first partition the set of k lower parity checks into two disjoint

subsets K1 and K2, of sizes k1 and k2 respectively, as illustrated in Fig. 2. Let H1 and H2 denote

the corresponding partitions of the full parity check matrix H ∈{0, 1}[k][×][m]. Now let us set all parity

bits in the subset K2 equal to zero, and consider the LDGM/LDPC code C(G, H1) defined by the

generator matrix G and the parity check (sub)matrix H1, as follows

C(G, H1) : = {x ∈{0, 1}[n] | x = Gy for some y ∈{0, 1}[m] such that H1 y = 0} . (5)

Note that the rate of C(G, H1) is given by R[′] = RG RH1, which can be suitably adjusted by

modifying the LDGM and LDPC rates respectively. Moreover, by applying Theorems 1 and 2,

there exist finite choices of degree such that C(G, H1) will be optimal for both source and channel

coding.

Considering now the remaining k2 parity bits in the subset K2, suppose that we set them equal

to a fixed binary sequence m ∈{0, 1}[k][2] . Now consider the code


. (6)


�
H1
H2


�


C(m) : =


�

x ∈{0, 1}[n] | x = Gy for some y ∈{0, 1}[m] such that


y =


� ��
0


m


Note that for each binary sequence m ∈{0, 1}[k][2], the code C(m) is a subcode of C(G, H1); moreover,

the collection of these subcodes forms a disjoint partition as follows

�
C(G, H1) = C(m). (7)

m∈{0,1}[k][2]

Again, Theorems 1 and 2 guarantee that (with suitable degree choices), each of the subcodes C(m)

is optimal for both source and channel coding. Thus, the LDGM/LDPC construction has a natural

nested property, in which a good source/channel code—namely C(G, H1)—is partitioned into a

disjoint collection {C(m), m ∈{0, 1}[k][1] } of good source/channel codes. We now illustrate how this

nested structure can be exploited for coding with side information.

10


-----



#### 4.2 Source coding with side information

We begin by showing that the compound construction can be used to perform source coding with

side information (SCSI).

4.2.1 Problem formulation

Suppose that we wish to compress a symmetric Bernoulli source S ∼ Ber( [1]2 [) so as to be able]

to reconstruct it with Hamming distortion D. As discussed earlier in Section 2, the minimum

achievable rate is given by R(D) = 1 − h (D). In the Wyner-Ziv extension of standard lossy com
pression [45], there is an additional source of side information about S—say in the form Z = S ⊕ W

where W ∼ Ber(δ) is observation noise—that is available only at the decoder. See Fig. 3 for a block

diagram representation of this problem.

S
�
Decoder
S R

Z

W

Figure 3. Block diagram representation of source coding with side information (SCSI). A source S is
compressed to rate R. The decoder is given the compressed version, and side information Z = S ⊕W,
and wishes to use (S, Z[�] ) to reconstruct the source S up to distortion D.

For this binary version of source coding with side information (SCSI), it is known [2] that the

minimum achievable rate takes the form

RWZ(D, p) = l. c. e. �h (D ∗ p) − h (D), (p, 0)�, (8)

where l. c. e. denotes the lower convex envelope. Note that in the special case p = 2[1] [, the side]

information is useless, so that the Wyner-Ziv rate reduces to classical rate-distortion. In the

discussion to follow, we focus only on achieving rates of the form h (D ∗ p)−h (D), as any remaining

rates on the Wyner-Ziv curve (8) can be achieved by time-sharing with the point (p, 0).

4.2.2 Coding procedure for SCSI

In order to achieve rates of the form R = h (D ∗ p) − h (D), we use the compound LDGM/LDPC

construction, as illustrated in Fig. 2, according to the following procedure.

Step #1, Source coding: The first step is a source coding operation, in which we transform the

source sequence S to a quantized representation S. In order to do so, we use the code C(G, H1), as

defined in equation (5) and illustrated in Fig. 4(a), composed of the generator matrix G and the

11


-----



parity check matrix H1. Note that C(G, H1), when viewed as a code with blocklength n, has rate

R1 : = m �1n− [k]m[1] � = m−nk1 . Suppose that we choose[1] the middle and lower layer sizes m and k1

respectively such that

m − k1
R1 = = 1 − h (D) + ǫ/2, (9)

n


where ǫ > 0 is arbitrary. For any such choice, Theorem 1 guarantees the existence of finite degrees

(dc, dv, d[′]c[) such that that][ C][(][G, H][1][) is a good][ D][-distortion source code. Consequently, for the speci-]

fied rate R1, we can use C(G, H1) in order to transform the source to some quantized representation
S such that the error S ⊕ S has average Hamming weighted bounded by D. Moreover, since S is a
� � �

codeword of C(G, H1), there is some sequence of information bits Y[�] ∈{0, 1}[m] such that S[�] = GY[�]

and H1Y[�] = 0.


n


dc
G

m dv
H1

k1 d[′]c

0 0 0

k1


n


dc
G

m dv
H1 H2

k d[′]c

0 0 0 1 0

k1 k2


(a) (b)

Figure 4. (a) Source coding stage for Wyner-Ziv procedure: the C(G, H1), specified by the generator
matrix G ∈{0, 1}[n][×][m] and parity check matrix H1 ∈{0, 1}[k][1][×][m], is used to quantize the source vector
S ∈{0, 1}[n], thereby obtaining a quantized version S[�] ∈{0, 1}[n] and associated vector of information
bits Y[�] ∈{0, 1}[m], such that S[�] = G Y[�] and H1 Y[�] = 0.

Step #2. Channel coding: Given the output (Y,[�] S) of the source coding step, consider the

[�]

sequence H2Y[�] ∈{0, 1}[k][2] of parity bits associated with the parity check matrix H2. Transmitting

this string of parity bits requires rate Rtrans = [k]n[2] [. Overall, the decoder receives both these][ k][2][ parity]

bits, as well as the side information sequence Z = S ⊕ W . Using these two pieces of information,

the goal of the decoder is to recover the quantized sequence S.

[�]

Viewing this problem as one of channel coding, the effective rate of this channel code is


R2 = [m][−][k]n[1][−][k][2] . Note that the side information can be written in the form

Z = S ⊕ W = S ⊕ E ⊕ W,

[�]

1Note that the choices of m and k1 need not be unique.

12


-----



where E : = S⊕S[�] is the quantization noise, and W ∼ Ber(p) is the channel noise. If the quantization

noise E were i.i.d. Ber(D), then the overall effective noise E ⊕ W would be i.i.d. Ber(D ∗ p). (In

reality, the quantization noise is not exactly i.i.d. Ber(D), but it can be shown [47] that it can be

treated as such for theoretical purposes.) Consequently, if we choose k2 such that

m − k1 − k2
R2 = = 1 − h (D ∗ p) − ǫ/2, (10)

n

for an arbitrary ǫ > 0, then Theorem 2 guarantees that the decoder will (w.h.p.) be able to recover

a codeword corrupted by (D ∗ p)-Bernoulli noise.

Summarizing our findings, we state the following:

Corollary 1. There exist finite choices of degrees (dc, dv, d[′]c[)][ such that the compound LDGM/LDPC]

construction achieves the Wyner-Ziv bound.

Proof. With the source coding rate R1 chosen according to equation (9), the encoder will return a

quantization S with average Hamming distance to the source S of at most D. With the channel

[�]
coding rate R2 chosen according to equation (10), the decoder can with high probability recover

the quantization S. The overall transmission rate of the scheme is

[�]


k2
Rtrans =

n

m − k1
= − [m][ −] [k][1][ −] [k][2]

n n

= R1 − R2

= (1 − h (D) + ǫ/2) − (1 − h (D ∗ p) − ǫ/2)

= h (D ∗ p) − h (D) + ǫ.

Since ǫ > 0 was arbitrary, we have established that the scheme can achieve rates arbitrarily close

to the Wyner-Ziv bound.

#### 4.3 Channel coding with side information

We now show how the compound construction can be used to perform channel coding with side

information (CCSI).

13


-----



4.3.1 Problem formulation

In the binary information embedding problem, given a specified input vector V ∈{0, 1}[n], the

channel output Z ∈{0, 1}[n] is assumed to take the form

Z = V ⊕ S ⊕ W, (11)

where S is a host signal (not under control of the user), and W ∼ Ber(p) corresponds to channel

noise. The encoder is free to choose the input vector V ∈{0, 1}[n], subject to an average channel

constraint

1
≤ w, (12)
n [E][ [][∥][V][ ∥][1][]]

for some parameter w ∈ (0, [1]2 [].] The goal is to use a channel coding scheme that satisfies this

constraint (12) so as to maximize the number of possible messages m that can be reliably commu
nicated. Moreover, We write V ≡ Vm to indicate that each channel input is implicitly identified

with some underlying message m. Given the channel output Z = Vm ⊕ S ⊕ W, the goal of the de
coder is to recover the embedded message m. The capacity for this binary information embedding

problem [2] is given by

RIE(w, p) = u. c. e. �h (w) − h (p), (0, 0)�, (13)

where u. c. e. denotes the upper convex envelope. As before, we focus on achieving rates of the form

h (w) − h (p), since any remaining points on the curve (13) can be achieved via time-sharing with

the (0, 0) point.

W

Z m

�
m Encoder Decoder

Vm

S

Figure 5. Block diagram representation of channel coding with side information (CCSI). The
encoder embeds a message m into the channel input Vm, which is required to satisfy the average
channel constraint n[1] [E][[][∥][V][m][∥][1][]][ ≤] [w][. The channel produces the output][ Z][ =][ V][m][ ⊕] [S][ ⊕] [W] [, where][ S][ is a]

host signal known only to the encoder, and W ∼ Ber(p) is channel noise. Given the channel output
Y, the decoder outputs an estimate m of the embedded message.
�

14

|Encoder V m|Z m b Decoder|
|---|---|


-----



4.3.2 Coding procedure for CCSI

In order to achieve rates of the form R = h (w) − h (p), we again use the compound LDGM/LDPC

construction in Fig. 2, now according to the following two step procedure.

Step #1: Source coding: The goal of the first stage is to embed the message into the transmitted

signal V via a quantization process. In order to do so, we use the code illustrated in Fig. 6(a),

specified by the generator matrix G and parity check matrices H1 and H2. The set K1 of k1 parity

bits associated with the check matrix H1 remain fixed to zero throughout the scheme. On the other

hand, we use the remaining k2 lower parity bits associated with H2 to specify a particular message

m ∈{0, 1}[k][2] that the decoder would like to recover. In algebraic terms, the resulting code C(m)

has the form


.(14)


�
H1
H2


�


C(m) : =


�

x ∈{0, 1}[n] | x = Gy for some y ∈{0, 1}[m] such that


y =


� ��
0


m


Since the encoder has access to host signal S, it may use this code C(m) in order to quantize the

host signal. After doing so, the encoder has a quantized signal Sm ∈{0, 1}[n] and an associated

[�]

sequence Ym ∈{0, 1}[m] of information bits such that Sm = G Ym. Note that the quantized signal

[�] [�] [�]

(Y[�]m, Sm) specifies the message m in an implicit manner, since m = H2 Ym by construction of the

[�] [�]

code C(m).

Now suppose that we choose n, m and k such that


m − k1 − k2
R1 = = 1 − h (w) + ǫ/2 (15)

n

for some ǫ > 0, then Theorem 1 guarantees that there exist finite degrees (dc, dv, d[′]c[) such that]

the resulting code is a good w-distortion source code. Otherwise stated, we are guaranteed that

w.h.p, the quantization error E : = S ⊕ S[�] has average Hamming weight upper bounded by wn.

Consequently, we may set the channel input V equal to the quantization noise (V = E), thereby

ensuring that the average channel constraint (12) is satisfied.

Step #2, Channel coding: In the second phase, the decoder is given a noisy channel observation

of the form

Z = E ⊕ S ⊕ W = S ⊕ W, (16)

[�]

and its task is to recover S. In terms of the code architecture, the k1 lower parity bits remain set to

[�]

zero; the remaining k2 parity bits, which represent the message m, are unknown to the coder. The

resulting code, as illustrated illustrated in Fig. 6(b), can be viewed as channel code with effective

15


-----



dc
G

m dv
H1


n


dc
G

m dv
H1 H2

k d[′]c

0 0 0 1 0

m


n


(a) (b)

Figure 6. (a) Source coding step for binary information embedding. The message m ∈{0, 1}[k][2]

specifies a particular coset; using this particular source code, the host signal S is compressed to S[�],
and the quantization error E = S ⊕ S[�] is transmitted over the constrained channel. (b) Channel
coding step for binary information embedding. The decoder receives Z = S[�] ⊕ W where W ∼ Ber(p)
is channel noise, and seeks to recover S[�], and hence the embedded message m specifying the coset.

rate [m][−]n[k][1] . Now suppose that we choose k1 such that the effective code used by the decoder has

rate

m − k1
R2 = = 1 − h (p) − ǫ/2, (17)

n

for some ǫ > 0. Since the channel noise W is Ber(p) and the rate R2 chosen according to (17),

Theorem 2 guarantees that the decoder will w.h.p. be able to recover the pair S and Y . Moreover,

[�] [�]

by design of the quantization procedure, we have the equivalence m = H2 Y[�] so that a simple

syndrome-forming procedure allows the decoder to recover the hidden message.

Summarizing our findings, we state the following:

Corollary 2. There exist finite choices of degrees (dc, dv, d[′]c[)][ such that the compound LDGM/LDPC]

construction achieves the binary information embedding (Gelfand-Pinsker) bound.

Proof. With the source coding rate R1 chosen according to equation (15), the encoder will return

a quantization S of the host signal S with average Hamming distortion upper bounded by w.

[�]

Consequently, transmitting the quantization error E = S ⊕ S[�] will satisfy the average channel

constraint (12). With the channel coding rate R2 chosen according to equation (17), the decoder

can with high probability recover the quantized signal S, and hence the message m. Overall, the

[�]

scheme allows a total of 2[k][2] distinct messages to be embedded, so that the effective information

16


-----



embedding rate is


k2
Rtrans =

n

m − k1
= − [m][ −] [k][1][ −] [k][2]

n n

= R2 − R1

= (1 − h (p) − ǫ/2) − (1 − h (w) + ǫ/2)

= h (w) − h (p) + ǫ,

for some ǫ > 0. Thus, we have shown that the proposed scheme achieves the binary information

embedding bound (13).

### 5 Proof of source coding optimality

This section is devoted to the proof of the previously stated Theorem 1 on the source coding

optimality of the compound construction.

#### 5.1 Set-up

In establishing a rate-distortion result such as Theorem 1, perhaps the most natural focus is the

random variable

1
dn(S, C) : = (18)
n [min]x∈C [∥][x][ −] [S][∥][1][,]

corresponding to the (renormalized) minimum Hamming distance from a random source sequence

S ∈{0, 1}[n] to the nearest codeword in the code C. Rather than analyzing this random variable

directly, our proof of Theorem 1 proceeds indirectly, by studying an alternative random variable.

Given a binary linear code with N codewords, let i = 0, 1, 2, . . ., N − 1 be indices for the

different codewords. We say that a codeword X [i] is distortion D-good for a source sequence S if the

Hamming distance ∥X [i] ⊕ S∥1 is at most Dn. We then set the indicator random variable Z [i](D) = 1

when codeword X [i] is distortion D-good. With these definitions, our proof is based on the following

random variable:


Tn(S, C; D) : =


N −1
�

Z [i](D). (19)

i=0


Note that Tn(S, C; D) simply counts the number of codewords that are distortion D-good for a

source sequence S. Moreover, for all distortions D, the random variable Tn(S, C; D) is linked to

17


-----



dn(S, C) via the equivalence

P[Tn(S, C; D) > 0] = P[dn(S, C) ≤ D]. (20)

Throughout our analysis of P[Tn(S, C; D) > 0], we carefully track only its exponential behavior.

More precisely, the analysis to follow will establish an inverse polynomial lower bound of the

form P[Tn(S, C; D) > 0] ≥ 1/f (n) where f (·) collects various polynomial factors. The following

concentration result establishes that the polynomial factors in these bounds can be ignored:

Lemma 1 (Sharp concentration). Suppose that for some target distortion D, we have

P[Tn(S, C; D) > 0] ≥ 1/f (n), (21)

where f (·) is a polynomial function satisfying log f (n) = o(n). Then for all ǫ > 0, there exists a

fixed code C[¯] of sufficiently large blocklength n such that E[dn(S; C[¯] )] ≤ D + ǫ.

Proof. Let us denote the random code C as (C1, C2), where C1 denotes the random LDGM top

code, and C2 denotes the random LDPC bottom code. Throughout the analysis, we condition on

some fixed LDPC bottom code, say C2 = C[¯] 2. We begin by showing that the random variable

(dn(S, C) | C[¯] 2) is sharply concentrated. In order to do so, we construct a vertex-exposure martin
gale [33] of the following form. Consider a fixed sequential labelling {1, . . ., n} of the top LDGM

checks, with check i associated with source bit Si. We reveal the check and associated source

bit in a sequential manner for each i = 1, . . ., n, and so define a sequence of random variables

{U0, U1, . . ., Un} via U0 : = E[dn(S, C) | C[¯] 2], and

Ui : = E �dn(S, C) | S1, . . ., Si, C[¯] 2�, i = 1, . . ., n. (22)

By construction, we have Un = (dn(S, C) | C[¯] 2). Moreover, this sequence satisfies the following

bounded difference property: adding any source bit Si and the associated check in moving from

Ui−1 to Ui can lead to a (renormalized) change in the minimum distortion of at most ci = 1/n.

Consequently, by applying Azuma’s inequality [1], we have, for any ǫ > 0,

P ���(dn(S, C) | ¯C2) − E[dn(S, C) | ¯C2]�� ≥ ǫ� ≤ exp �−nǫ[2][�] . (23)

Next we observe that our assumption (21) of inverse polynomial decay implies that, for at least

one bottom code C[¯] 2,

P[dn(S, C) ≤ D | C[¯] 2] = P[Tn(S, C; D) > 0 | C[¯] 2] ≥ 1/g(n), (24)

18


-----



for some subexponential function g. Otherwise, there would exist some α > 0 such that

P[Tn(S, C; D) > 0 | C[¯] 2] ≤ exp(−nα)

for all choices of bottom code C[¯] 2, and taking averages would violate our assumption (21).

Finally, we claim that the concentration result (23) and inverse polynomial bound (24) yield

the result. Indeed, if for some ǫ > 0, we had D < E[dn(S, C) | C[¯] 2] − ǫ, then the concentration

bound (23) would imply that the probability

P[dn(S, C) ≤ D | C[¯] 2] ≤ P[dn(S, C) ≤ E[dn(S, C) | C[¯] 2] − ǫ | C[¯] 2]

≤ P ���(dn(S, C) | ¯C2) − E[dn(S, C) | ¯C2]�� ≥ ǫ�

decays exponentially, which would contradict the inverse polynomial bound (24) for sufficiently

large n. Thus, we have shown that assumption (21) implies that for all ǫ > 0, there exists a

sufficiently large n and fixed bottom code C[¯] 2 such that E[dn(S, C) | C[¯] 2] ≤ D + ǫ. If the average

over LDGM codes C1 satisfies this bound, then at least one choice of LDGM top code must also

satisfy it, whence we have established that there exists a fixed code C[¯] such that E[dn(S; C[¯] )] ≤ D+ǫ,

as claimed.

#### 5.2 Moment analysis

In order to analyze the probability P[Tn(S, C; D) > 0], we make use of the moment bounds given

in the following elementary lemma:

Lemma 2 (Moment methods). Given any random variable N taking non-negative integer values,


there holds
(E[N ])[2]

E[N [2]]


(a) (b)
≤ P[N > 0] ≤ E[N ]. (25)


Proof. The upper bound (b) is an immediate consequence of Markov’s inequality, whereas the lower

bound (a) follows by applying the Cauchy-Schwarz inequality [20] as follows

(E[N ])[2] = �E�N I[N > 0]��2 ≤ E[N [2]] E �I[2][N > 0]� = E[N [2]] P[N > 0].

The remainder of the proof consists in applying these moment bounds to the random variable

Tn(S, C; D), in order to bound the probability P[Tn(S, C; D) > 0]. We begin by computing the first

moment:

19


-----



Lemma 3 (First moment). For any code with rate R, the expected number of D-good codewords

scales exponentially as

1
= [R − (1 − h (D))] ± o(1). (26)
n [log][ E][[][T][n][]]

Proof. First, by linearity of expectation E[Tn] = i=0 P[Z [i](D) = 1] = 2[n][R]P[Z [0](D) = 1], where

[�][2][n][R][−][1]

we have used symmetry of the code construction to assert that P[Z [i](D) = 1] = P[Z [0](D) = 1] for

all indices i. Now the event {Z [0](D) = 1} is equivalent to an i.i.d Bernoulli( 2[1] [) sequence of length]

n having Hamming weight less than or equal to Dn. By standard large deviations theory (either

Sanov’s theorem [11], or direct asymptotics of binomial coefficients), we have

1
= 1 − h (D) ± o(1),
n [log][ P][[][Z] [0][(][D][) = 1]]

which establishes the claim.

Unfortunately, however, the first moment E[Tn] need not be representative of typical behavior

of the random variable Tn, and hence overall distortion performance of the code. As a simple

illustration, consider an imaginary code consisting of 2[n][R] copies of the all-zeroes codeword. Even

for this “code”, as long as R > 1 − h (D), the expected number of distortion-D optimal codewords

grows exponentially. Indeed, although Tn = 0 for almost all source sequences, for a small subset of

source sequences (of probability mass ≈ 2[−][n][ [1][−][h][(][D][)]]), the random variable Tn takes on the enormous

value 2[nR], so that the first moment grows exponentially. However, the average distortion incurred

by using this code will be ≈ 0.5 for any rate, so that the first moment is entirely misleading. In order

to assess the representativeness of the first moment, one needs to ensure that it is of essentially the

same order as the variance, hence the comparison involved in the second moment bound (25)(a).

#### 5.3 Second moment analysis

Our analysis of the second moment begins with the following alternative representation:


Lemma 4.


�� �[�]
1 + P[Z [j](D) = 1 | Z [0](D) = 1] . (27)

j=0̸


E[Tn[2][(][D][)] =][ E][[][T][n][(][D][)]]


�


Based on this lemma, proved in Appendix C, we see that the key quantity to control is the condi
tional probability P[Z [j](D) = 1 | Z [0](D) = 1]. It is this overlap probability that differentiates the

low-density codes of interest here from the unstructured codebooks used in classical random coding

20


-----



arguments.[2] For a low-density graphical code, the dependence between the events {Z [j](D) = 1}

and {Z [0](D) = 1} requires some analysis.

Before proceeding with this analysis, we require some definitions. Recall our earlier definition (3)

of the average weight enumerator associated with an (dv, d[′]c[) LDPC code, denoted by][ A][m][(][w][).]

Moreover, let us define for each w ∈ [0, 1] the probability

Q(w; D) : = P [∥X(w) ⊕ S∥1 ≤ Dn | ∥S∥1 ≤ Dn], (28)

where the quantity X(w) ∈{0, 1}[n] denotes a randomly chosen codeword, conditioned on its under
lying length-m information sequence having Hamming weight ⌈wm⌉. As shown in Lemma 9 (see

Appendix A), the random codeword X(w) has i.i.d. Bernoulli elements with parameter


1
δ[∗](w; dc) =
2


� �
1 − (1 − 2 w)[d][c] . (29)


With these definitions, we now break the sum on the RHS of equation (27) into m terms, indexed

by t = 1, 2, . . ., m, where term t represents the contribution of a given non-zero information sequence

y ∈{0, 1}[m] with (Hamming) weight t. Doing so yields

m

� �

P[Z [j](D) = 1 | Z [0](D) = 1] = Am(t/m) Q(t/m; D)
j=0̸ t=1

≤ m max
1≤t≤m [{][A][m][(][t/m][)][ Q][(][t/m][;][ D][)][}]

≤ m max
w∈[0,1] [{][A][m][(][w][)][ Q][(][w][;][ D][)][}][ .]

Consequently, we need to control both the LDPC weight enumerator Am(w) and the probability

Q(w; D) over the range of possible fractional weights w ∈ [0, 1].

#### 5.4 Bounding the overlap probability

The following lemma, proved in Appendix D, provides a large deviations bound on the probability
Q(w; D).

Lemma 5. For each w ∈ [0, 1], we have

1
≤ F (δ[∗](w; dc); D) + o(1), (30)
n [log][ Q][(][w][;][ D][)]

2In the latter case, codewords are chosen independently from some ensemble, so that the overlap probability is
simply equal to P[Z[j] (D) = 1]. Thus, for the simple case of unstructured random coding, the second moment bound
actually provides the converse to Shannon’s rate-distortion theorem for the symmetric Bernoulli source.

21


-----



where for each t ∈ (0, [1]

2 []][ and][ D][ ∈] [(0][,][ 1]2 []][, the error exponent is given by]

� � �
F (t; D) : = D log (1 − t)e[λ][∗] + t + (1 − D) log (1 − t) + te[λ][∗][�] − λ[∗]D. (31)


� √
Here λ[∗] : = log −b+ 2ba[2]−4ac


�
, where a : = t (1 − t) (1 − D), b : = (1 − 2D)t[2], and c : = −t (1 − t) D.


In general, for any D ∈ (0, 2[1] [], the function][ F] [(][ ·][ ;][ D][) has the following properties. At][ t][ = 0,]

it achieves its maximum F (0 ; D) = 0, and then is strictly decreasing on the interval (0, 2[1] [], ap-]

proaching its minimum value − [1 − h (D)] as t → 2[1] [. Figure 7 illustrates the form of the function]

F (δ[∗](ω; dc); D) for two different values of distortion D, and for degrees dc ∈{3, 4, 5}. Note that


Decay of overlap probability: D = 0.1101


Decay of overlap probability: D = 0.3160


0

−0.1


0

−0.1


−0.2

−0.3


−0.2

−0.3


−0.4

−0.5


−0.4

−0.5


−0.6
0 0.1 0.2 0.3 0.4 0.5

Weight


−0.6
0 0.1 0.2 0.3 0.4 0.5

Weight

|Col1|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
|||||||
|||||||
|||||||
|d = c d = c||3 4||||
||d = c d = c|3 4||||
|d = c|d = c|5||||

|Col1|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
|||||||
|||||||
|||||||
|d = c d = c||3 4||||
||d = c d = c|3 4||||
|d = c|d = c|5||||


(a) (b)

Figure 7. Plot of the upper bound (30) on the overlap probability n[1] [log][ Q][(][w][;][ D][) for different choices]

of the degree dc, and distortion probabilities. (a) Distortion D = 0.1100. (b) Distortion D = 0.3160.


increasing dc causes F (δ[∗](ω; dc); D) to approach its minimum −[1 − h (D)] more rapidly.

We are now equipped to establish the form of the effective rate-distortion function for any


compound LDGM/LDPC ensemble. Substituting the alternative form of E[Tn[2][] from equation (27)]

into the second moment lower bound (25) yields


  

1 1  � 
≥ log E[Tn(D)] − log P[Z [j](D) = 1 | Z [0](D) = 1] 
n [log][ P][[][T][n][(][D][)][ >][ 0]] n

[1 +] j=0̸ 

� 1 �

≥ R − (1 − h (D)) − max − o(1)
w∈[0,1] n [log][ A][m][(][w][) + 1]n [log][ Q][(][w][;][ D][)]

� 1 log Am(w) �

≥ R − (1 − h (D)) − max R + F ( δ[∗](w; dc), D) − o(1), (32)
w∈[0,1] RH m

22


-----



Minimum achievable rates: (R,D) = (0.50, 0.1100)


Minimum achievable rates: (R,D) = (0.10, 0.3160)


0.6

0.5


0.125

0.1


0.4

0.3


0.075

0.05


0.2

0.1


0
0 0.1 0.2 0.3 0.4 0.5

Weight


0.025

0
0 0.1 0.2 0.3 0.4 0.5

Weight

|Col1|Col2|Col3|Col4|C|ompound|
|---|---|---|---|---|---|
||||N|N|aive LDGM|
|||||||
|||||||
|||||||
|||||||
|||||||
|||||||

|Col1|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
||||C N|C N|ompound aive LDGM|
|||||||
|||||||
|||||||
|||||||
|||||||


(a) (b)


Figure 8. Plot of the function defining the lower bound (33) on the minimum achievable rate for a
specified distortion. Shown are curves with LDGM top degree dc = 4, comparing the uncoded case
(no bottom code, dotted curve) to a bottom (4, 6) LDPC code (solid line). (a) Distortion D = 0.1100.
(b) Distortion D = 0.3160.

where the last step follows by applying the upper bound on Q from Lemma 5, and the relation

m = RGn = RRH [n][. Now letting][ B][(][w][;][ d][v][, d]c[′] [) be any upper bound on the log of average weight]

enumerator [log][ A][m][(][w][)], we can then conclude that [1]

m n [log][ P][[][T][n][(][D][)][ >][ 0] is asymptotically non-negative]

for all rate-distortion pairs (R, D) satisfying


�


R ≥ max
w∈[0,1]


�
1 − h (D) + F (δ[∗](w; dc), D)

1 − [B][(][w]R[;][d]H[v][,d][′][c][)]


. (33)


Figure 8 illustrates the behavior of the RHS of equation (33), whose maximum defines the effective

rate-distortion function, for the case of LDGM top degree dc = 4. Panels (a) and (b) show the

cases of distortion D = 0.1100 and D = 0.3160 respectively, for which the respective Shannon rates

are R = 0.50 and R = 0.10. Each panel shows two plots, one corresponding the case of uncoded

information bits (a naive LDGM code), and the other to using a rate RH = 2/3 LDPC code with
degrees (dv, dc) = (4, 6). In all cases, the minimum achievable rate for the given distortion is

obtained by taking the maximum for w ∈ [0, 0.5] of the plotted function. For any choices of D, the

plotted curve is equal to the Shannon bound RSha = 1 − h (D) at w = 0, and decreases to 0 for

w = [1]

2 [.]

Note the dramatic difference between the uncoded and compound constructions (LDPC-coded).


In particular, for both settings of the distortion (D = 0.1100 and D = 0.3160), the uncoded curves

rise from their initial values to maxima above the Shannon limit (dotted horizontal line). Con
23


-----



sequently, the minimum required rate using these constructions lies strictly above the Shannon

optimum. The compound construction curves, in contrast, decrease monotonically from their max
imum value, achieved at w = 0 and corresponding to the Shannon optimum. In the following

section, we provide an analytical proof of the fact that for any distortion D ∈ [0, [1]

2 [), it is al-]

ways possible to choose finite degrees such that the compound construction achieves the Shannon

optimum.

#### 5.5 Finite degrees are sufficient

In order to complete the proof of Theorem 1, we need to show that for all rate-distortion pairs

(R, D) satisfying the Shannon bound, there exist LDPC codes with finite degrees (dv, d[′]c[) and a]

suitably large but finite top degree dc such that the compound LDGM/LDPC construction achieves

the specified (R, D).

Our proof proceeds as follows. Recall that in moving from equation (32) to equation (33), we

assumed a bound on the average weight enumerator Am of the form

1
≤ B(w; dv, d[′]c[) +][ o][(1)][.] (34)
m [log][ A][m][(][w][)]

For compactness in notation, we frequently write B(w), where the dependence on the degree pair

(dv, d[′]c[) is understood implicitly. In the following paragraph, we specify a set of conditions on this]

bounding function B, and we then show that under these conditions, there exists a finite degree dc
such that the compound construction achieves specified rate-distortion point. In Appendix F, we

then prove that the weight enumerator of standard regular LDPC codes satisfies the assumptions

required by our analysis.

Assumptions on weight enumerator bound We require that our bound B on the weight

enumerator satisfy the following conditions:

A1: the function B is symmetric around [1]2 [, meaning that][ B][(][w][) =][ B][(1][ −] [w][) for all][ w][ ∈] [[0][,][ 1].]


A2: the function B is twice differentiable on (0, 1) with B[′]( [1]2 [) = 0 and][ B][′′][(] [1]2 [)][ <][ 0.]

A3: the function B achieves its unique optimum at w = [1]

2 [, where][ B][(] [1]2 [) =][ R][H][.]


A4: there exists some ǫ1 > 0 such that B(w) < 0 for all w ∈ (0, ǫ1), meaning that the ensemble

has linear minimum distance.

24


-----



In order to establish our claim, it suffices to show that for all (R, D) such that R > 1 − h (D),

there exists a finite choice of dc such that


max
w∈[0,1]


 
 

+ F (δ[∗](w; dc), D) ≤ R − [1 − h (D)] : = ∆ (35)

[R B]R[(]H[w][)] 

� �� �

K(w; dc)


Restricting to even dc ensures that the function F is symmetric about w = 2[1] [; combined with as-]

sumption A2, this ensures that K is symmetric around [1]

2 [, so that we may restrict the maximization]

to [0, [1]

2 [] without loss of generality. Our proof consists of the following steps:]

(a) We first prove that there exists an ǫ1 > 0, independent of the choice of dc, such that

K(w; dc) ≤ ∆for all w ∈ [0, ǫ1].

(b) We then prove that there exists ǫ2 > 0, again independent of the choice of dc, such that

K(w; dc) ≤ ∆for all w ∈ [ 2[1] [−] [ǫ][2][,][ 1]2 [].]


(c) Finally, we specify a sufficiently large but finite degree d[∗]c [that ensures the condition][ K][(][w][;][ d]c[∗][)][ ≤] [∆]

for all w ∈ [ǫ1, ǫ2].

5.5.1 Step A

By assumption A4 (linear minimum distance), there exists some ǫ1 > 0 such that B(w) ≤ 0 for all

w ∈ [0, ǫ1]. Since F (δ[∗](w; dc); D) ≤ 0 for all w, we have K(w; dc) ≤ 0 < ∆in this region. Note

that ǫ1 is independent of dc, since it specified entirely by the properties of the bottom code.

5.5.2 Step B

For this step of the proof, we require the following lemma on the properties of the function F :

Lemma 6. For all choices of even degrees dc ≥ 4, the function G(w; dc) = F (δ[∗](w; dc), D) is
differentiable in a neighborhood of w = [1]

2 [, with]


G( [1] G[′]( [1] and G[′′]( [1] (36)

2 [;][ d][c][) =][ −] [[1][ −] [h][ (][D][)]][,] 2 [;][ d][c][) = 0][,] 2 [;][ d][c][) = 0][.]

See Appendix E for a proof of this claim. Next observe that we have the uniform bound


G(w; dc) ≤ G(w; 4) for all dc ≥ 4 and w ∈ [0, 2[1] [].] This follows from the fact that F (u; D) is

decreasing in u, and that δ[∗](w; 4) ≤ δ[∗](w; dc) for all dc ≥ 4 and w ∈ [0, [1]2 []. Since][ B][ is independent]

of dc, this implies that K(w; dc) ≤ K(w; 4) for all w ∈ [0, [1]2 []. Hence it suffices to set][ d][c][ = 4, and]

show that K(w; 4) ≤ ∆for all w ∈ [ [1]2 [−] [ǫ][2][,][ 1]2 []. Using Lemma 6, Assumption A2 concerning the]

25


-----



derivatives of B, and Assumption A4 (that B( [1]

2 [) =][ R][H][), we have]

K( [1] = R − [1 − h (D)] = ∆,

2 [; 4)]

R B[′]( 2[1] [)]

K [′]( [1] = + G[′]( [1] and

2 [; 4)] RH 2 [; 4) = 0][,]

R B[′′]( 2[1] [)] 2 [)]

K [′′]( [1] = + G[′′]( [1] < 0.

2 [; 4)] RH 2 [; 4) =][ R B]RH[′′][(] [1]


By the continuity of K [′′], the second derivative remains negative in a region around [1]2 [, say for all]

w ∈ [ [1]2 [−] [ǫ][2][,][ 1]2 [] for some][ ǫ][2][ >][ 0. Then, for all][ w][ ∈] [[] 2[1] [−] [ǫ][2][,][ 1]2 [], we have for some][ �][w][ ∈] [[][w,][ 1]2 [] the second]

order expansion


�

K(w; 4) = K( [1] w − [1]

2 [; 4) +][ K] [′][(1]2 [; 4)(][w][ −] 2[1] [) + 1]2 [K][(][w][ �][; 4)] 2


�2


�

= ∆+ [1] w − [1]

2 [K][(][w][ �][; 4)] 2


�2
≤ ∆.


Thus, we have established that there exists an ǫ2 > 0, independent of the choice of dc, such that

for all even dc ≥ 4, we have

K(w; dc) ≤ K(w, 4) ≤ ∆ for all w ∈ [ [1]2 [−] [ǫ][2][,][ 1]2 [].] (37)


5.5.3 Step C

Finally, we need to show that K(w; dc) ≤ ∆for all w ∈ [ǫ1, ǫ2]. From assumption A3 and the

continuity of B, there exists some ρ(ǫ2) > 0 such that

B(w) ≤ RH [1 − ρ(ǫ2)] for all w ≤ [1]2 [−] [ǫ][2][.] (38)


From Lemma 6, limu→ 21 [F] [(][u][;][ D][) =][ F] [(] 2[1] [;][ D][) =][ −] [[1][ −] [h][ (][D][)]. Moreover, as][ d][c][ →] [+][∞][, we have]

δ[∗](ǫ1; dc) → [1]2 [. Therefore, for any][ ǫ][3][ >][ 0, there exists a finite degree][ d]c[∗] [such that]

F (δ[∗](ǫ1; d[∗]c [);][ D][)] ≤ − [1 − h (D)] + ǫ3.

26


-----



Since F is non-increasing in w, we have F (δ[∗](w; d[∗]c [);][ D][)][ ≤−] [[1][ −] [h][ (][D][)] +][ ǫ][3] [for all][ w][ ∈] [[][ǫ][1][, ǫ][2][].]

Putting together this bound with the earlier bound (38) yields that for all w ∈ [ǫ1, ǫ2]:

K(w; dc) = R [B][(][w][)] + F (δ[∗](w; d[∗]c [)][, D][)]

RH

≤ R [1 − ρ(ǫ2)] − [1 − h (D)] + ǫ3

= {R − [1 − h (D)]} + (ǫ3 − Rρ(ǫ2))

= ∆+ (ǫ3 − Rρ(ǫ2))


Since we are free to choose ǫ3 > 0, we may set ǫ3 = [R][ρ]2[(][ǫ][2][)] to yield the claim.

### 6 Proof of channel coding optimality

In this section, we turn to the proof of the previously stated Theorem 2, concerning the channel

coding optimality of the compound construction.

If the codeword x ∈{0, 1}[n] is transmitted, then the receiver observes V = x ⊕ W, where W

is a Ber(p) random vector. Our goal is to bound the probability that maximum likelihood (ML)

decoding fails where the probability is taken over the randomness in both the channel noise and

the code construction. To simplify the analysis, we focus on the following sub-optimal (non-ML)

decoding procedure. Let ǫn be any non-negative sequence such that ǫn/n → 0 but ǫ[2]n[/n][ →] [+][∞][—say]

for instance, ǫn = n[2][/][3].

Definition 2 (Decoding Rule:). With the threshold d(n) : = pn + ǫn, decode to codeword xi ⇐⇒

∥xi ⊕ V ∥1 ≤ d(n), and no other codeword is within d(n) of V .

The extra term ǫn in the threshold d(n) is chosen for theoretical convenience. Using the following

two lemmas, we establish that this procedure has arbitrarily small probability of error, whence ML

decoding (which is at least as good) also has arbitrarily small error probability.

Lemma 7. Using the suboptimal procedure specified in the definition (2), the probability of decoding

error vanishes asymptotically provided that

RG B(w) − D (p||δ[∗](w; dc) ∗ p) < 0 for all w ∈ (0, 2[1] []][,] (39)

where B is any function bounding the average weight enumerator as in equation (34).

Proof. Let N = 2[nR] = 2[mR][H] denote the total number of codewords in the joint LDGM/LDPC

code. Due to the linearity of the code construction and symmetry of the decoding procedure, we

may assume without loss of generality that the all zeros codeword 0[n] was transmitted (i.e., x = 0[n]).

In this case, the channel output is simply V = W and so our decoding procedure will fail if and

only if one the following two conditions holds:

27


-----



(i) either ∥W ∥1 > d(n), or

(ii) there exists a sequence of information bits y ∈{0, 1}[m] satisfying the parity check equation

Hy = 0 such that the codeword Gy satisfies ∥Gy ⊕ W ∥1 ≤ d(n).

Consequently, using the union bound, we can upper bound the probability of error as follows:


perr ≤ P[∥W ∥1 > d(n)] +


N
� P�∥Gy[i] ⊕ W ∥1 ≤ d(n)�. (40)

i=2


Since E[∥W ∥1] = pn, we may apply Hoeffdings’s inequality [13] to conclude that


�
P[∥W ∥1 > d(n)] ≤ 2 exp −2 [ǫ]n[2]
n


�
→ 0 (41)


by our choice of ǫn. Now focusing on the second term, let us rewrite it as a sum over the possible

Hamming weights ℓ = 1, 2, . . ., m of information sequences (i.e., ∥y∥1 = ℓ) as follows:


N
� P�∥Gy[i] ⊕ W ∥1 ≤ d(n)� =

i=2


m
� Am( [ℓ] �∥Gy ⊕ W ∥1 ≥ d(n) �� ∥y∥1 = ℓ�,

m [)][ P]

ℓ=1


where we have used the fact that the (average) number of information sequences with fractional

weight ℓ/m is given by the LDPC weight enumerator Am( m[ℓ] [). Analyzing the probability terms]

in this sum, we note Lemma 9 (see Appendix A) guarantees that Gy has i.i.d. Ber(δ[∗]( m[ℓ] [;][ d][c][))]

elements, where δ[∗]( · ; dc) was defined in equation (29). Consequently, the vector Gy ⊕ W has i.i.d.

Ber(δ( m[ℓ] [)][ ∗] [p][) elements. Applying Sanov’s theorem [11] for the special case of binomial variables]

yields that for any information bit sequence y with ℓ ones, we have

P�∥Gy ⊕ W ∥1 ≥ d(n) �� ∥y∥1 = ℓ� ≤ f (n)2[−][nD][(][p][||][δ][(][ ℓ]m [)][∗][p][)], (42)

for some polynomial term f (n). We can then upper bound the second term in the error bound (40)

as


N
� P�∥Gy[i] ⊕ W ∥1 ≤ d(n)� ≤ f (m) exp

i=2


�


max
1≤ℓ≤m


�

� ��[�]
mB( [ℓ] p||δ( [ℓ],

m [) +][ o][(][m][)][ −] [nD] m [)][ ∗] [p]


where we have used equation (42), as well as the assumed upper bound (34) on Am in terms of B.

Simplifying further, we take logarithms and rescale by m to assess the exponential rate of decay,

28


-----



thereby obtaining


1
m [log]


N
� P�∥Gy[i] ⊕ W ∥1 ≤ d(n)� ≤ max

1≤ℓ≤m
i=2

≤ max
w∈[0,1]


� � ��
B( [ℓ] D p||δ( [ℓ] + o(1)

m [)][ −] R[1]G m [)][ ∗] [p]

� �
B(w) − [1] D (p||δ(w) ∗ p) + o(1),

RG


and establishing the claim.

Lemma 8. For any p ∈ (0, 1) and total rate R : = RG RH < 1 − h (p), there exist finite choices

of the degree triplet (dc, dv, d[′]c[)][ such that][ (39)][ is satisfied.]

Proof. For notational convenience, we define

L(w) : = RGB(w) − D (p||δ[∗](w; dc) ∗ p) . (43)

First of all, it is known [17] that a regular LDPC code with rate RH = [d]d[v][′]c [<][ 1 and][ d][v][ ≥] [3 has linear]

minimum distance. More specifically, there exists a threshold ν[∗] = ν[∗](dv, dc) such that B(w) ≤ 0

for all w ∈ [0, ν[∗]]. Hence, since B(w) − D (p||δ[∗](w; dc) ∗ p) ≥ 0 for all w ∈ (0, 1), for w ∈ (0, ν[∗]], we

have L(w) < 0.


Turning now to the interval [ν[∗], [1]2 [], consider the function]

L�(w) : = Rh (w) − D (p||δ[∗](w; dc) ∗ p) . (44)


Since B(w) ≤ RHh (w), we have L(w) ≤ L[�](w), so that it suffices to upper bound L[�]. Observe that
L�( [1]2 [) =][ R][ −] [(1][ −] [h][ (][p][))][ <][ 0 by assumption. Therefore, it suffices to show that, by appropriate]

choice of dc, we can ensure that L(w) ≤ L[�]( 2[1] [). Noting that][ �][L][ is infinitely differentiable, calculating]

[�]

derivatives yields L[′]( [1] (See Appendix G for details of these derivative

[�] 2 [) = 0 and][ �][L][′′][(] [1]2 [)][ <][ 0.]

calculations.) Hence, by second order Taylor series expansion around w = [1]

2 [, we obtain]

L(w) = L( [1] L[′′]( ¯w)(w − [1]
� � �

2 [) + 1]2 2 [)][2][,]


where ¯w ∈ [w, 2[1] []. By continuity of][ �][L][′′][, we have][ �][L][′′][(][w][)][ <][ 0 for all][ w][ in some neighborhood of][ 1]2 [,]

so that the Taylor series expansion implies that L(w) ≤ L[�]( [1]2 [) for all][ w][ in some neighborhood, say]

[�]

(µ, [1]2 [].]

It remains to bound L on the interval [ν[∗], µ]. On this interval, we have L(w) ≤ Rh (µ) −

[�] [�]

D (p||δ[∗](ν[∗]; dc) ∗ p). By examining equation (29) from Lemma 9, we see that by choosing dc
sufficiently large, we can make δ[∗](ν[∗]; dc) arbitrarily close to 21 [, and hence][ D][ (][p][||][δ][∗][(][ν][∗][;][ d][c][)][ ∗] [p][)]

arbitrarily close to 1 − h (p). More precisely, let us choose dc large enough to guarantee that

D (p||δ[∗](ν[∗]; dc) ∗ p) < (1 − ǫ) (1 − h (p)), where ǫ = [R][ (1]1−[−]h[h](p[(][µ]) [))] . With this choice, we have, for all

29


-----



w ∈ [ν[∗], µ], the sequence of inequalities

L�(w) ≤ Rh (µ) − D (p||δ[∗](ν[∗]; dc) ∗ p)

< Rh (µ) − �(1 − h (p)) − R(1 − h (µ))�

= R − (1 − h (p)) < 0,

which completes the proof.

### 7 Discussion

In this paper, we established that it is possible to achieve both the rate-distortion bound for

symmetric Bernoulli sources and the channel capacity for the binary symmetric channel using

codes with bounded graphical complexity. More specifically, we have established that there exist

low-density generator matrix (LDGM) codes and low-density parity check (LDPC) codes with

finite degrees that, when suitably compounded to form a new code, are optimal for both source

and channel coding. To the best of our knowledge, this is the first demonstration of classes of codes
with bounded graphical complexity that are optimal as source and channel codes simultaneously.

We also demonstrated that this compound construction has a naturally nested structure that can

be exploited to achieve the Wyner-Ziv bound [45] for lossy compression of binary data with side

information, as well as the Gelfand-Pinsker bound [19] for channel coding with side information.

Since the analysis of this paper assumed optimal decoding and encoding, the natural next step

is the development and analysis of computationally efficient algorithms for encoding and decoding.

Encouragingly, the bounded graphical complexity of our proposed codes ensures that they will, with

high probability, have high girth and good expansion, thus rendering them well-suited to message
passing and other efficient decoding procedures. For pure channel coding, previous work [16, 36, 41]

has analyzed the performance of belief propagation when applied to various types of compound

codes, similar to those analyzed in this paper. On the other hand, for pure lossy source coding, our

own past work [44] provides empirical demonstration of the feasibility of modified message-passing

schemes for decoding of standard LDGM codes. It remains to extend both these techniques and their

analysis to more general joint source/channel coding problems, and the compound constructions

analyzed in this paper.

#### Acknowledgements

The work of MJW was supported by National Science Foundation grant CAREER-CCF-0545862,

a grant from Microsoft Corporation, and an Alfred P. Sloan Foundation Fellowship.

30


-----



### A Basic property of LDGM codes

For a given weight w ∈ (0, 1), suppose that we enforce that the information sequence y ∈{0, 1}[m]

has exactly ⌈wm⌉ ones. Conditioned on this event, we can then consider the set of all codewords

X(w) ∈{0, 1}[n], where we randomize over low-density generator matrices G chosen as in step (a)

above. Note for any fixed code, X(w) is simply some codeword, but becomes a random variable

when we imagine choosing the generator matrix G randomly. The following lemma characterizes

this distribution as a function of the weight w and the LDGM top degree dc:

Lemma 9. Given a binary vector y ∈{0, 1}[m] with a fraction w of ones, the distribution of

the random LDGM codeword X(w) induced by y is i.i.d. Bernoulli with parameter δ[∗](w; dc) =

� �

21 1 − (1 − 2 w)[d][c] .

Proof. Given a fixed sequence y ∈{0, 1}[m] with a fraction w ones, the random codeword bit Xi(w)

at bit i is formed by connecting dc edges to the set of information bits.[3] Each edge acts as an i.i.d.

Bernoulli variable with parameter w, so that we can write

Xi(w) = V1 ⊕ V2 ⊕ . . . ⊕ Vdc, (45)

where each Vk ∼ Ber(w) is independent and identically distributed. A straightforward calculation

using z-transforms (see [17]) or Fourier transforms over GF (2) yields that Xi(w) is Bernoulli with
parameter δ[∗](w; dc) as defined.

### B Bounds on binomial coefficients

The following bounds on binomial coefficients are standard (see Chap. 12, [11]):


� k
h

n


� �n� � k
− [log(][n][ + 1)] ≤ [1] ≤ h

n n [log] k n


�
. (46)


Here, for α ∈ (0, 1), the quantity h(α) : = −α log α − (1 − α) log(1 − α) is the binomial entropy

function.

3In principle, our procedure allows two different edges to choose the same information bit, but the probability of
such double-edges is asymptotically negligible.

31


-----



### C Proof of Lemma 4

First, by the definition of Tn(D), we have


N −1

E[Tn[2][(][D][)]] = E  �

i=1


N −1 
�

Z [i](D)Z [j](D)



j=0


�

P[Z [i](D) = 1, Z [i](D) = 1].

j≠ i


= E[Tn] +


N −1
�

i=0


To simplify the second term on the RHS, we first note that for any i.i.d Bernoulli( 2[1] [) sequence]

S ∈{0, 1}[n] and any codeword X [j], the binary sequence S[′] : = S ⊕ X [j] is also i.i.d. Bernoulli( 2[1] [).]

Consequently, for each pair i ̸= j, we have

P �Z [i](D) = 1, Z [j](D) = 1� = P �∥X [i] ⊕ S∥1 ≤ Dn, ∥X [j] ⊕ S∥1 ≤ Dn�

= P �∥X [i] ⊕ S[′]∥1 ≤ Dn, ∥X [j] ⊕ S[′]∥1 ≤ Dn�

= P �∥X [i] ⊕ X [j] ⊕ S∥1 ≤ Dn, ∥S∥1 ≤ Dn� .

Note that for each j ̸= i, the vector X [i] ⊕ X [j] is a non-zero codeword. For each fixed i, summing

over j ̸= i can be recast as summing over all non-zero codewords, so that


� P �Z [i](D) = 1, Z [j](D) = 1� =

i≠ j


N −1
�

i=0

N −1
�

i=0


� P �∥X [i] ⊕ X [j] ⊕ S∥1 ≤ Dn, ∥S∥1 ≤ Dn�

j≠ i


�− � � �

= P ∥X [k] ⊕ S∥1 ≤ Dn, ∥S∥1 ≤ Dn

i=0 k=0̸

� �

= 2[n][R][ �] P ∥X [k] ⊕ S∥1 ≤ Dn, ∥S∥1 ≤ Dn

k=0̸

� �

= 2[n][R]P �Z [0](D) = 1� [�] P Z [k](D) = 1 | Z [0](D) = 1

k=0̸

� � �
= E[Tn] P Z [k](D) = 1 | Z [0](D)

k=0̸


thus establishing the claim.


32


-----



### D Proof of Lemma 5

We reformulate the probability Q(w, D) as follows. Recall that Q involves conditioning the source

sequence S on the event ∥S∥1 ≤ Dn. Accordingly, we define a discrete variable T with distribution


P(T = t) =


�n�
t for t = 0, 1, . . ., Dn,

�Dn �n�
s=0 s


representing the (random) number of 1s in the source sequence S. Let Ui and Vj denote Bernoulli

random variables with parameters 1 − δ[∗](w; dc) and δ[∗](w; dc) respectively. With this set-up, con
ditioned on codeword j having a fraction wn ones, the quantity Q(w, D) is equivalent to the

probability that the random variable


�Ti=1 [U][j][ +][ �]j[n]=1[−][T] [V][j] if T ≥ 1
(47)
�nj=1 [V][j] if T = 0


W : =








is less than Dn. To bound this probability, we use a Chernoff bound in the form


1
≤ inf
n [log][ P][[][W][ ≤] [Dn][]] λ<0


� 1 �

. (48)
n [log][ M][W] [(][λ][)][ −] [λD]


We begin by computing the moment generating function MW . Taking conditional expectations and

using independence, we have


MW (λ) =


Dn
�

P[T = t] [MU (λ)][t] [MV (λ)][n][−][t] .
t=0


Here the cumulant generating functions have the form

� �
log MU (λ) = log (1 − δ)e[λ] + δ, and (49a)

�
log MV (λ) = log (1 − δ) + δe[λ][�], (49b)

where we have used (and will continue to use) δ as a shorthand for δ[∗](w; dc).

Of interest to us is the exponential behavior of this expression in n. Using the standard entropy

approximations to the binomial coefficient (see Appendix B), we can bound MW (λ) as


f (n)


Dn � t � � �
� exp �n�h − h (D) + [t] 1 − [t] log MV (λ)��, (50)

n n [log][ M][U] [(][λ][) +] n

t=0

� �� �

g(t)

33


-----



where f (n) denotes a generic polynomial factor. Further analyzing this sum, we have


1
n [log]


Dn
� 1

g(t) ≤ max + [log(][nD][)]
n 0≤t≤Dn [log][ g][(][t][) + log][ f]n[(][n][)] n
t=0

� � t � �

= max h − h (D) + [t] 1 − [t]
0≤t≤Dn n n [log][ M][U] [(][λ][) +] n


� �
log MV (λ) + o(1)


≤ max
u∈[0,D] [{][h][ (][u][)][ −] [h][ (][D][) +][ u][ log][ M][U] [(][λ][) + (1][ −] [u][) log][ M][V][ (][λ][)][}][ +][ o][(1)][.]

Combining this upper bound on [1]

n [log][ M][W] [(][λ][) with the Chernoff bound (48) yields that]

1
≤ inf (51)
n [log][ P][[][W][ ≤] [Dn][]] λ<0 u[max]∈[0,D] [G][(][u, λ][;][ δ][) +][ o][(1)]

where the function G takes the form

G(u, λ; δ) : = h (u) − h (D) + u log MU (λ) + (1 − u) log MV (λ) − λD. (52)

Finally, we establish that the solution (u[∗], λ[∗]) to the min-max saddle point problem (51) is

unique, and specified by u[∗] = D and λ[∗] as in Lemma 5. First of all, observe that for any δ ∈ (0, 1),

the function G is continuous, strictly concave in u and strictly convex in λ. (The strict concavity

follows since h (u) is strictly concave with the remaining terms linear; the strict convexity follows

since cumulant generating functions are strictly convex.) Therefore, for any fixed λ < 0, the

maximum over u ∈ [0, D] is always achieved. On the other hand, for any D > 0, u ∈ [0, D] and

δ ∈ (0, 1), we have G(u; λ; t) → +∞ as λ →−∞, so that the infimum is either achieved at some

λ[∗] < 0, or at λ[∗] = 0. We show below that it is always achieved at an interior point λ[∗] < 0. Thus

far, using standard saddle point theory [21], we have established the existence and uniqueness of

the saddle point solution (u[∗], λ[∗]).

To verify the fixed point conditions, we compute partial derivatives in order to find the optimum.

First, considering u, we compute


∂G

= log [1][ −] [u] + log MU (λ) − log MV (λ)
∂u [(][u, λ][;][ δ][)] u

� � �

= log [1][ −] [u] + log (1 − δ)e[λ] + δ − log (1 − δ) + δe[λ][�] .

u


Solving the equation [∂G]

∂u [(][u, λ][;][ δ][) = 0 yields]

exp(λ) 1
u[′] = (53)
1 + exp(λ) [D][ +] 1 + exp(λ) [(1][ −] [D][)][ ≥] [0][.]


Since D ≤ [1]2 [, a bit of algebra shows that][ u][′][ ≥] [D][ for all choices of][ λ][. Since the maximization is]

34


-----



constrained to [0, D], the optimum is always attained at u[∗] = D.

Turning now to the minimization over λ, we compute the partial derivative to find

∂G (1 − δ) exp(λ) δ exp(λ)

= u
∂λ [(][u, λ][;][ δ][)] (1 − δ) exp(λ) + δ [+ (1][ −] [u][)] (1 − δ) + δ exp(λ) [−] [D.]

Setting this partial derivative to zero yields a quadratic equation in exp(λ) with coefficients

a = δ (1 − δ) (1 − D) (54a)

b = u(1 − δ)[2] + (1 − u)δ[2] − D �δ[2] + (1 − δ)[2][�] . (54b)

c = −Dδ(1 − δ). (54c)

The unique positive root ρ[∗] of this quadratic equation is given by


1
ρ[∗](δ, D, u) : =
2a


� �
−b +


�
b[2] − 4ac . (55)


It remains to show that ρ[∗] ≤ 1, so that λ[∗] : = log ρ[∗] < 0. A bit of algebra (using the fact a ≥ 0)

shows that ρ[∗] < 1 if and only if a + b + c > 0. We then note that at the optimal u[∗] = D, we have

b = (1 − 2D)δ[2], whence

a + b + c = δ (1 − δ) (1 − D) + (1 − 2D)δ[2] − Dδ(1 − δ)

= (1 − 2D) δ > 0

since D < [1]2 [and][ δ >][ 0. Hence, the optimal solution is][ λ][∗] [: = log][ ρ][∗] [<][ 0, as specified in the lemma]

statement.

### E Proof of Lemma 6

A straightforward calculation yields that


G( [1]

2 [) =][ F] [(][δ][∗][(1]2 [;][ d][c][);][ D][) =][ F] [(1]2 [;][ D][) =][ −] [(1][ −] [h][ (][D][))]

as claimed. Turning next to the derivatives, we note that by inspection, the solution λ[∗](t) defined in

Lemma 5 is twice continuously differentiable as a function of t. Consequently, the function F (t, D)

is twice continuously differentiable in t. Moreover, the function δ[∗](w; dc) is twice continuously

differentiable in w. Overall, we conclude that G(w) = F (δ[∗](w; dc); D) is twice continuously differ
entiable in w, and that we can obtain derivatives via chain rule. Computing the first derivative,

35


-----



we have


G[′]( [1] = δ[′]( [1]

2 [)] 2 [)][ F][ ′][(][δ][∗][(1]2 [;][ d][c][);][ D][) = 0]

since δ[′](w) = −dc (1 − 2w)[d][c][−][1], which reduces to zero at w = 2[1] [. Turning to the second derivative,]


we have

� �2

G[′′]( [1] = δ[′′]( [1] δ[′]( [1] F [′′](δ[∗]( [1]

2 [)] 2 [)][ F][ ′][(][δ][∗][(1]2 [;][ d][c][);][ D][) +] 2 [)] 2 [;][ d][c][);][ D][) =][ δ][′′][(1]2 [)][ F][ ′][(][δ][∗][(1]2 [;][ d][c][);][ D][)][.]


We again compute δ[′′](w) = 2dc (dc − 1)(1 − 2w)[d][c][−][2], which again reduces to zero at w = 2[1] [since]

dc ≥ 4 by assumption.

### F Regular LDPC codes are sufficient

Consider a regular (dv, d[′]c[) code from the standard Gallager LDPC ensemble. In order to complete]

the proof of Theorem 1, we need to show for suitable choices of degree (dv, d[′]c[), the average weight]

enumerator of these codes can be suitably bounded, as in equation (34), by a function B that

satisfies the conditions specified in Section 5.5.

It can be shown [17, 22] that for even degrees d[′]c[, the average weight enumerator of the regular]

Gallager ensemble, for any block length m, satisfies the bound

1
= B(w; dv, d[′]c[) +][ o][(1)][.]
m [log][ A][m][(][w][)]


The function B in this relation is defined for w ∈ [0, [1]

2 [] as]


B(w; dv, d[′]c[)] : = (1 − dv)h (w) − (1 − RH ) + dv inf
λ≤0


� 1 log �(1 + e[λ])[d]c[′] + (1 − e[λ])[d]c[′] � − wλ�, (56)

d[′]c


and by B(w) = B(w − [1]2 [) for][ w][ ∈] [[] [1]2 [,][ 1]. Given that the minimization problem (56) is strictly]

convex, a straightforward calculation of the derivative shows the optimum is achieved at λ[∗], where

λ[∗] ≤ 0 is the unique solution of the equation

c[−][1] − (1 − e[λ])[d]c[′] [−][1]
e[λ][ (1 +][ e][λ][)][d][′] = w. (57)

(1 + e[λ])[d][′][c] + (1 − e[λ])[d][′][c]

Some numerical computation for RH = 0.5 and different choices (dv, d[′]c[) yields the curves shown]

in Fig. 9.

We now show that for suitable choices of degree (dv, d[′]c[), the function][ B][ defined in equation (56)]

satisfies the four assumptions specified in Section 5.5. First, for even degrees d[′]c[, the function][ B]

36


-----



LDPC weight enumerators


0.6

0.5


0.4

0.3


0.2

0.1


0 c

−0.1
0 0.1 0.2 0.3 0.4 0.5

Weight

|Col1|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
|||||||
|||||||
|||||||
|||d||= 6||
||||d|= 6||
||||d d|c = 8 c = 10 c||
|||||||


Figure 9. Plots of LDPC weight enumerators for codes of rate RH = 0.5, and check degrees
d[′]c [∈{][6][,][ 8][,][ 10][}][.]

is symmetric about w = 2[1] [, so that assumption (A1) holds. Secondly, we have][ B][(][w][)][ ≤] [R][H] [, and]

moreover, for w = [1]2 [, the optimal][ λ][∗][(] [1]2 [) = 0, so that][ B][(] 2[1] [) =][ R][H][, and assumption (A3) is satisfied.]

Next, it is known from the work of Gallager [17], and moreover is clear from the plots in Fig. 9,


that LDPC codes with dv > 2 have linear minimum distance, so that assumption (A4) holds.

The final condition to verify is assumption (A2), concerning the differentiability of B. We


summarize this claim in the following:

Lemma 10. The function B is twice continuously differentiable on (0, 1), and in particular we


have


B[′]( [1] and B[′′]( [1] (58)

2 [) = 0][,] 2 [)][ <][ 0][.]


Proof. Note that for each fixed w ∈ (0, 1), the function

f (λ) = 1 log �(1 + e[λ])[d]c[′] + (1 − e[λ])[d]c[′] � = 1 log �(e[−][λ] + 1)[d]c[′] + (e[−][λ] − 1)[d]c[′] � + λ
d[′]c d[′]c


is strictly convex and twice continuously differentiable as a function of λ. Moreover, the function

f [∗](w) : = infλ≤0 {f (λ) − λw} corresponds to the conjugate dual [21] of f (λ) + I≤0(λ). Since the

optimum is uniquely attained for each w ∈ (0, 1), an application of Danskin’s theorem [4] yields

that f [∗] is differentiable with dwd [f][ ∗][(][w][) =][ −][λ][∗][(][w][), where][ λ][∗] [is defined by equation (57). Putting]

together the pieces, we have B[′](w) = (1 − dv)h[′](w) − dvλ[∗](w). Evaluating at w = 12 [yields]

B[′]( 2[1] [) = 0][ −] [d][v][λ][∗][(0) = 0 as claimed.]

We now claim that λ[∗](w) is differentiable. Indeed, let us write the defining relation (57) for


λ[∗](w) as F (λ, w) = 0 where F (λ, w) : = f [′](λ) − w. Note that F is twice continuously differentiable

37


-----



in both λ and w; moreover, [∂F]

∂λ [exists for all][ λ][ ≤] [0 and][ w][, and satisfies][ ∂F]∂λ [(][λ, w][) =][ f][ ′′][(][λ][)][ >][ 0]

by the strict convexity of f . Hence, applying the implicit function theorem [4] yields that λ[∗](w)

dλ[∗]
is differentiable, and moreover that Hence, combined with our earlier

dw [(][w][) = 1][/f][ ′′][(][λ][∗][(][w][)).]

calculation of B[′], we conclude that B[′′](w) = (1 − dv)h[′′](w) − dv f [′′](λ1(w)) [.] Our final step is to

compute the second derivative f [′′]. In order to do so, it is convenient to define g = log f [′], and

exploit the relation g[′]f [′] = f [′′]. By definition, we have

� c[−][1] c[−][1][�] � c c �
g(λ) = λ + log (1 + e[λ])[d][′] − (1 − e[λ])[d][′] − log (1 + e[λ])[d][′] + (1 − e[λ])[d][′]


whence

c[−][2] + (1 − e[λ])[d]c[′] [−][2] (1 + e[λ])[d]c[′] [−][1] − (1 − e[λ])[d]c[′] [−][1]
g[′](λ) = 1 + e[λ](d[′]c [−] [1)(1 +][ e][λ][)][d][′] c

(1 + e[λ])[d][′][c][−][1] − (1 − e[λ])[d][′][c][−][1][ −] [e][λ][d][′] (1 + e[λ])[d][′][c] + (1 − e[λ])[d][′][c]


Evaluating at w = 2[1] [corresponds to][ λ][(0) = 0, so that]


= [1]

4 [.]


c[−][2] 2[d]c[′] [−][1]
1 + (d[′]c [−] [1)2][d][′] c

2[d][′][c][−][1][ −] [d][′] 2[d][′][c]


�


f [′′](λ( [1] = f [′](0) g[′](0) = [1]

2 [))] 2


�


Consequently, combining all of the pieces, we have

1

B[′′](w) = (1 − dv)h[′′]( [1] − 4dv < 0

2 [)][ −] [d][v] f [′′](λ( [1]2 [)) =][ d][v][ −]4 [1]


as claimed.

### G Derivatives of L
 [�]

Here we calculate the first and second derivatives of the function L defined in equation (44). The

[�]

first derivative takes the form


δ[′](v; dc)

L[′](v) = R log [1][ −] [v] + p [δ][′][(][v][;][ d][c][)]
�

v δ(v; dc) [−] [(1][ −] [p][)] 1 − δ(v; dc)

38


-----



where δ[′](v; dc) = dc(1 − 2v)[d][c][−][1]. Since δ[′]( 2[1] [;][ d][c][) = 0, we have][ �][L][′][(] [1]2 [) = 0 as claimed. Second, using]

chain rule, we calculate

1
L[′′](v) = −R� � + p [δ][′′][(][v][;][ d][c][)][δ][(][v][;][ d][c][)][ −] [[][δ][′][(][v][;][ d][c][)]][2]
�
1 − v [+ 1]v [δ(v; dc)][2]

�1 − δ(v; dc)� + [δ[′](v; dc)][2]
− (1 − p) [δ][′′][(][v][;][ d][c][)]

[1 − δ(v; dc)][2]


and δ[′′](v; dc) = −dc (dc − 1) (1 − 2v)[d][c][−][2]. Now for dc > 2, we have δ[′′]( [1]2 [) = 0, so that][ �][L][′′][(] [1]2 [) =]

−4R < 0 as claimed.

### References

[1] N. Alon and J. Spencer. The Probabilistic Method. Wiley Interscience, New York, 2000.

[2] R. J. Barron, B. Chen, and G. W. Wornell. The duality between information embedding and source
coding with side information and some applications. IEEE Trans. Info. Theory, 49(5):1159–1180, 2003.

[3] C. Berroux and A. Glavieux. Near optimum error correcting coding and decoding: Turbo codes. IEEE
Trans. Commun., 44:1261–1271, October 1996.

[4] D. Bertsekas. Nonlinear programming. Athena Scientific, Belmont, MA, 1995.

[5] J. Chou, S. S. Pradhan, and K. Ramchandran. Turbo coded trellis-based constructions for data embedding: Channel coding with side information. In Proceedings of the Asilomar Conference, November
2001.

[6] J. Chou, S. S. Pradhan, and K. Ramchandran. Turbo and trellis-based constructions for source coding
with side information. In Proceedings of the Data Compression Conference (DCC), 2003.

[7] S.-Y. Chung, G. D. Forney, T. Richardson, and R. Urbanke. On the design of low-density parity-check
codes within 0.0045 dB of the Shannon limit. IEEE Communications Letters, 5(2):58–60, February
2001.

[8] S. Ciliberti and M. M´ezard. The theoretical capacity of the parity source coder. Technical report,
[August 2005. arXiv:cond-mat/0506652.](http://arxiv.org/abs/cond-mat/0506652)

[9] S. Ciliberti, M. M´ezard, and R. Zecchina. Message-passing algorithms for non-linear nodes and data
[compression. Technical report, November 2005. arXiv:cond-mat/0508723.](http://arxiv.org/abs/cond-mat/0508723)

[10] S. Cocco, O. Dubois, J. Mandler, and R. Monasson. Rigorous decimation-based construction of ground
pure states for spin-glass models on random lattices. Physical Review Letters, 90(4), January 2003.

[11] T. Cover and J. Thomas. Elements of Information Theory. John Wiley and Sons, New York, 1991.

[12] N. Creignou, H. Daud/’e, and O. Dubois. Approximating the satisfiability threshold of random XOR
formulas. Combinatorics, Probability and Computing, 12:113–126, 2003.

[13] L. Devroye, L. Gyorfi, and G. Lugosi. A Probabilistic Theory of Pattern Recognition. Springer-Verlag,
New York, 1996.

[14] O. Dubois and J. Mandler. The 3-XORSAT threshold. In Proc. 43rd Symp. FOCS, pages 769–778,
2002.

[15] U. Erez and S. ten Brink. A close-to-capacity dirty paper coding scheme. IEEE Trans. Info. Theory,
51(10):3417–3432, 2005.

[16] O. Etesami and A. Shokrollahi. Raptor codes on binary memoryless symmetric channels. IEEE Trans.
on Information Theory, 52(5):2033–2051, 2006.

[17] R. G. Gallager. Low-density parity check codes. MIT Press, Cambridge, MA, 1963.

[18] J. Garcia-Frias and Y. Zhao. Compression of binary memoryless sources using punctured turbo codes.
IEEE Communication Letters, 6(9):394–396, September 2002.

39


-----



[19] S. I. Gelfand and M. S. Pinsker. Coding for channel with random parameters. Probl. Pered. Inform.
(Probl. Inf. Tranmission), 9(1):19–31, 1983.

[20] G. Grimmett and D. Stirzaker. Probability and Random Processes. Oxford Science Publications, Clarendon Press, Oxford, 1992.

[21] J. Hiriart-Urruty and C. Lemar´echal. Convex Analysis and Minimization Algorithms, volume 1.
Springer-Verlag, New York, 1993.

[22] S. Litsyn and V. Shevelev. On ensembles of low-density parity-check codes: asymptotic distance distributions. IEEE Trans. Info. Theory, 48(4):887–908, April 2002.

[23] A. Liveris, Z. Xiong, and C. Georghiades. Nested convolutional/turbo codes for the binary Wyner-Ziv
problem. In Proceedings of the International Conference on Image Processing (ICIP), volume 1, pages
601–604, September 2003.

[24] H. A. Loeliger. An introduction to factor graphs. IEEE Signal Processing Magazine, 21:28–41, 2004.

[25] M. Luby, M. Mitzenmacher, M. A. Shokrollahi, and D. Spielman. Improved low-density parity check
codes using irregular graphs. IEEE Trans. Info. Theory, 47:585–598, February 2001.

[26] M. W. Marcellin and T. R. Fischer. Trellis coded quantization of memoryless and Gauss-Markov
sources. IEEE Trans. Communications, 38(1):82–93, 1990.

[27] E. Martinian and M. J. Wainwright. Analysis of LDGM and compound codes for lossy compression
and binning. In Workshop on Information Theory and Applications (ITA), February 2006. Available
[at arxiv:cs.IT/0602046.](http://arxiv.org/abs/cs/0602046)

[28] E. Martinian and M. J. Wainwright. Low density codes achieve the rate-distortion bound. In Data
Compression Conference, volume 1, March 2006. Available at arxiv:cs.IT/061123.

[29] E. Martinian and M. J. Wainwright. Low density codes can achieve the Wyner-Ziv and GelfandPinsker bounds. In International Symposium on Information Theory, July 2006. Available at
[arxiv:cs.IT/0605091.](http://arxiv.org/abs/cs/0605091)

[30] E. Martinian and J. Yedidia. Iterative quantization using codes on graphs. In Allerton Conference on
Control, Computing, and Communication, October 2003.

[31] Y. Matsunaga and H. Yamamoto. A coding theorem for lossy data compression by LDPC codes. IEEE
Trans. Info. Theory, 49:2225–2229, 2003.

[32] M. M´ezard, F. Ricci-Tersenghi, and R. Zecchina. Alternative solutions to diluted p-spin models and
XORSAT problems. Jour. of Statistical Physics, 111:105, 2002.

[33] R. Motwani and P. Raghavan. Randomized Algorithms. Cambridge University Press, Cambridge, UK,
1995.

[34] T. Murayama. Thouless-Anderson-Palmer approach for lossy compression. Physical Review E,
69:035105(1)–035105(4), 2004.

[35] T. Murayama and M. Okada. One step RSB scheme for the rate distortion function. J. Phys. A: Math.
Gen., 65:11123–11130, 2003.

[36] H. Pfister, I. Sason, and R. Urbanke. Capacity-achieving ensembles for the binary erasure channel with
bounded complexity. IEEE Trans. on Information Theory, 51(7):2352–2379, 2005.

[37] S. S. Pradhan and K. Ramchandran. Distributed source coding using syndromes (DISCUS): Design
and construction. IEEE Trans. Info. Theory, 49(3):626–643, 2003.

[38] T. Richardson, A. Shokrollahi, and R. Urbanke. Design of capacity-approaching irregular low-density
parity check codes. IEEE Trans. Info. Theory, 47:619–637, February 2001.

[39] T. Richardson and R. Urbanke. The capacity of low-density parity check codes under message-passing
decoding. IEEE Trans. Info. Theory, 47:599–618, February 2001.

[40] D. Schonberg, S. S. Pradhan, and K. Ramchandran. LDPC codes can approach the slepian-wolf bound
for general binary sources. In Proceedings of the 40th Annual Allerton Conference on Control, Communication, and Computing, pages 576–585, October 2002.

[41] A. Shokrollahi. Raptor codes. IEEE Trans. on Information Theory, 52(6):2551–2567, 2006.

[42] Y. Sun, A. Liveris, V. Stankovic, and Z. Xiong. Near-capacity dirty-paper code designs based on TCQ
and IRA codes. In ISIT, September 2005.

[43] A. J. Viterbi and J. K. Omura. Trellis encoding of memoryless discrete-time sources with a fidelity
criterion. IEEE Trans. Info. Theory, IT-20(3):325–332, 1974.

40


-----



[44] M. J. Wainwright and E. Maneva. Lossy source coding by message-passing and decimation over generalized codewords of LDGM codes. In International Symposium on Information Theory, Adelaide,
[Australia, September 2005. Available at arxiv:cs.IT/0508068.](http://arxiv.org/abs/cs/0508068)

[45] A. D. Wyner and J. Ziv. The rate-distortion function for source encoding with side information at the
encoder. IEEE Trans. Info. Theory, IT-22:1–10, January 1976.

[46] Y. Yang, V. Stankovic, Z. Xiong, and W. Zhao. On multiterminal source code design. In Proceedings
of the Data Compression Conference, 2005.

[47] R. Zamir, S. S. (Shitz), and U. Erez. Nested linear/lattice codes for structured multiterminal binning.
IEEE Trans. Info. Theory, 6(48):1250–1276, 2002.

41


-----

