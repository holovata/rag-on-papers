=== PROMPT ===
You are a knowledgeable assistant.
Provide a factually rich and accurate answer using the context below. If you are confident, you may include relevant background knowledge not found in the context.
Cite sources from the context when possible using (source: filename.md, chunk N).

Query:
What conditions must be satisfied for a graph to be perfectly Markovian, and how does this affect the construction of k-graphs from a probability distribution?

Context:
Source: 0705.1613.md (chunk 2)
Content: there are no other independencies present in the probability distribution than those represented
by the graph. This property is called the perfect Markovianity of the probability distribution
with respect to the associated concentration graph. We prove in this paper that this particular
concentration graph, the one associated with a perfect Markov distribution, can be determined
by only conditioning on a limited number of variables. We demonstrate that this number is
equal to the maximum size of the minimal separators in the concentration graph.

Source: 0705.1613.md (chunk 7)
Content: If V \ (A ∪ B ∪ S) separates A and B in G0, then XA ⊥⊥ XB|XS. (2)

Let P be a probability distribution belonging to a certain graphical model (X,G, F ).
The probability distribution P is said to be perfectly Markov to G if the converse of the
global Markov property (1) is also satisfied, that is, for any triple of disjoint non-empty
subsets (A,B,S) where S is not empty,

A⊥CB|S ⇐⇒ XA ⊥⊥ XB|XS. (3)

It was conjectured in Geiger and Pearl (1993) that for any undirected graph G we can
find a Gaussian probability distribution P that is perfectly Markov to G. In the Gaussian


-----



Determining full conditional independence by low-order conditioning 1181

case the perfect Markovianity assumption is equivalent to the following property: For all
non-adjacent vertices α and β in V and for all S ⊆ V \ {α,β} and S ̸= ∅,

S separates α and β ⇐⇒ Xα ⊥⊥ Xβ|XS. (4)

Source: 0705.1613.md (chunk 36)
Content: #### 4. Conclusion

In this paper we have proved that a concentration graph model can be determined using
a limited number of conditioning variables. The cardinality of this limited subset is
determined by looking at the structure of the undirected graph associated with the
corresponding distribution global Markov property. Certainly the perfect Markovianity
assumption is also needed for our result to be valid. Our result remains true for both
continuous and discrete distributions.
Our result can also be used as a justification of the estimation of graphical models by low-order conditioning such as using the PC algorithm (see Spirtes et al.
(2000), Kalisch and B¨uhlmann (2007), Kjærulff and Madsen (2007)), the 0–1 procedure
(see Friedman et al. (2000) and Wille and B¨uhlman (2006)), or the qp-procedure (see
Castello and Roverato (2006)). Practical applications of these procedures are useful when
the number of observations are far fewer than the number of variables. We first estimate

Source: 0705.1613.md (chunk 1)
Content: A concentration graph associated with a random vector is an undirected graph where each
vertex corresponds to one random variable in the vector. The absence of an edge between any
pair of vertices (or variables) is equivalent to full conditional independence between these two
variables given all the other variables. In the multivariate Gaussian case, the absence of an edge
corresponds to a zero coefficient in the precision matrix, which is the inverse of the covariance
matrix.
It is well known that this concentration graph represents some of the conditional independencies in the distribution of the associated random vector. These conditional independencies
correspond to the “separations” or absence of edges in that graph. In this paper we assume that
there are no other independencies present in the probability distribution than those represented
by the graph. This property is called the perfect Markovianity of the probability distribution

Source: 0705.1613.md (chunk 26)
Content: Gm. 
Lemma 6. Let (X,G, F ) be a concentration graph model and let P be a probability distribution belonging to F . For m,k ∈ N, let Gm and Gk be two undirected graphs constructed
using (5). If P is perfectly Markov to G and 1 ≤ m ≤ k ≤|V | − 2 then Ek ⊆ Em.

=== ANSWER ===
To answer your question, a graph is perfectly Markovian if it satisfies the following conditions:

1. The converse of the global Markov property (1) holds for any triple of disjoint non-empty subsets (A,B,S) where S is not empty: A⊥CB|S ⇐⇒ XA ⊥⊥ XB|XS.
2. For all non-adjacent vertices α and β in V and for all S ⊆ V \ {α,β} and S ̸= ∅, the separation condition holds: S separates α and β ⇐⇒ Xα ⊥⊥ Xβ|XS.

These conditions ensure that the graph represents the conditional independencies present in the probability distribution. In other words, if there is an edge between two vertices in the graph, then the corresponding variables are not fully independent given all the other variables.

The construction of k-graphs from a probability distribution relies on these conditions. Specifically, if P is perfectly Markov to G and 1 ≤ m ≤ k ≤|V | − 2, then Ek ⊆ Em, where Ek and Em are two undirected graphs constructed using the concentration graph model.

This result has implications for the estimation of graphical models by low-order conditioning, such as using the PC algorithm, the 0–1 procedure, or the qp-procedure. These procedures can be used to estimate the concentration graph associated with a probability distribution, which in turn can be used to determine the conditional independencies present in the distribution.

In the context of Gaussian distributions, perfect Markovianity is equivalent to the condition that for all non-adjacent vertices α and β in V and for all S ⊆ V \ {α,β} and S ̸= ∅, S separates α and β ⇐⇒ Xα ⊥⊥ Xβ|XS.

Overall, perfect Markovianity is a strong assumption that ensures the graph represents the conditional independencies present in the probability distribution. It has important implications for the construction of k-graphs from a probability distribution and for the estimation of graphical models by low-order conditioning.
