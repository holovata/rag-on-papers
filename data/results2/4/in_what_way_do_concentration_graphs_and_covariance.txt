=== PROMPT ===
You are a clear and helpful assistant.
Write an easy-to-read answer that summarizes the context naturally. Use your own knowledge to clarify if needed, and minimize direct citations unless they are critical.

Query:
In what way do concentration graphs and covariance graphs differ in representing conditional independence, and how is this distinction formalized in terms of separation criteria?

Context:
Source: 0705.1613.md (chunk 5)
Content: the associated graphical model is called a concentration graph model (see Lauritzen
(1996)). Dempster (1972) studied concentration graph models for Gaussian distributions under the name of covariance selection or covariance selection models. The absence of an edge between a given pair of vertices (α,β) in the associated graph indicates that the random variable Xα is independent of Xβ given all the other variables
X−αβ = (Xγ,γ ̸= α,β)[′]. These models are very well studied, especially the ones corresponding to Gaussian distributions (see Whittaker (1990), Lauritzen (1996), Edwards
(2000) and, recently, Letac and Massam (2007) and Rajaratnam et al. (2008)). The separation criteria defined on such graphs is a simple separation criteria on undirected graphs:
S ⊆ V separates two disjoint non-empty subsets A and B of V if any path joining a vertex
in A and another in B intersects S.
Other graphical models are represented by graphs with bi-directed edges. These models

Source: 0705.1613.md (chunk 9)
Content: (α,β) /∈ E ⇐⇒ Xα ⊥⊥ Xβ | X−αβ,

and if A, B and S are three disjoint non-empty subsets of V

S separates A and B in G ⇐⇒ XA ⊥⊥ XB|XS.

The aim of this paper is to prove the existence of a relationship between the cardinality
of the separators in G and the maximum number of conditioning variables needed to
determine the full conditional independencies in P . We proceed first by defining a new
parameter for undirected graphs, called the “separability order ”. Subsequently we prove
that when we condition on a fixed number of variables equal to this separability order,
the graph that is obtained is exactly the concentration graph.
The paper is organized as follows. Section 2 is devoted to the definition of this parameter and of some properties thereof. In Section 3, we will define a sequence of undirected graphs constructed due to conditional independencies for a given fixed number
k ∈{0,..., |V | − 2}. More precisely, we define the k-graph Gk = (V,Ek) by

Source: 0705.1613.md (chunk 1)
Content: A concentration graph associated with a random vector is an undirected graph where each
vertex corresponds to one random variable in the vector. The absence of an edge between any
pair of vertices (or variables) is equivalent to full conditional independence between these two
variables given all the other variables. In the multivariate Gaussian case, the absence of an edge
corresponds to a zero coefficient in the precision matrix, which is the inverse of the covariance
matrix.
It is well known that this concentration graph represents some of the conditional independencies in the distribution of the associated random vector. These conditional independencies
correspond to the “separations” or absence of edges in that graph. In this paper we assume that
there are no other independencies present in the probability distribution than those represented
by the graph. This property is called the perfect Markovianity of the probability distribution

Source: 0705.1613.md (chunk 30)
Content: concentration graph model G is equal to the k-partial graph G[p]k [as defined in (][6][) when]
k is smaller than the separability order of G, referred to in that paper as the “outer
connectivity of the missing edges”. The result in Theorem 4, however, is based on a
construction of a sequence of nested graphs. It starts from the covariance graph, that is,
G0, the 0-graph, and it becomes stationary and equal to the concentration graph when
the number of conditioning variables is equal to the separability order of the concentration
graph.
In Lemma 8, we show that k-partial graphs and k-graphs are equal when the perfect
Markovianity assumption is satisfied. Next, we give a corollary of Theorem 4. In Corollary 9 we give a condition that allows us to determine the last undirected k-graph in
the sequence of nested graphs obtained due to Theorem 4. This condition is given in
term of the degree two of the k-graphs, not as in Castello and Roverato (2006), where

Source: 0705.1613.md (chunk 10)
Content: (α,β) /∈ Ek ⇐⇒ ∃S ⊆ V \ {α,β} such that |S| = k and Xα ⊥⊥ Xβ|XS. (5)

When k = 0, the conditional independence given an empty set corresponds to the
marginal independence between Xα and Xβ. In the case when k = 0, the corresponding k-graph is then denoted by G0 and constructed using the pairwise Markov property
with respect to bi-directed graphs (see Cox and Wermuth (1996) and Chaudhuri et al.
(2007)). We mean that

α ̸∼G0 β ⇐⇒ Xα ⊥⊥ Xβ.

The graph G0 is also called a covariance graph (see Chaudhuri et al. (2007)). Wille and
B¨uhlman (2006) define a graph called a 0–1 graph, which corresponds to a graph with a
set of edges equal to E0 ∩ E1. We will show later (see Lemma 7) that this graph is equal
to G1. Castello and Roverato (2006) consider so called k-partial graphs G[p]k [= (][V,E]k[p][),]
which are defined as follows:

α ̸∼Gk β ⇐⇒ ∃S ⊆ V \ {α,β} such that |S| ≤ k and Xα ⊥⊥ Xβ|XS. (6)

=== ANSWER ===
Let's break down the key differences between concentration graphs and covariance graphs in representing conditional independence.

**Concentration Graphs:**

* Represent full conditional independence between variables given all other variables.
* An absence of an edge between two vertices (α, β) indicates that Xα is independent of Xβ.
* The graph represents some of the conditional independencies in the distribution of the associated random vector.
* These models are well-studied, especially for Gaussian distributions.

**Covariance Graphs:**

* Represent marginal independence between variables (i.e., no edge means Xα and Xβ are independent).
* A covariance graph is a special case of a concentration graph where k = 0.
* The graph represents the "separations" or absence of edges in that graph.

The key distinction lies in how they represent conditional independence. Concentration graphs focus on full conditional independence given all other variables, while covariance graphs represent marginal independence between variables.

In terms of separation criteria, concentration graphs use a simple separation criterion on undirected graphs: any path joining a vertex in one subset and another in the other subset must intersect with the separator set S.

Covariance graphs, being a special case of concentration graphs, also follow this separation criterion. However, when k > 0, the condition for conditional independence changes to ∃S ⊆ V \ {α,β} such that |S| = k and Xα ⊥⊥ Xβ|XS.

The separability order is an important concept in concentration graphs, which refers to the maximum number of conditioning variables needed to determine full conditional independencies. This parameter is used to construct a sequence of nested graphs, starting from the covariance graph (0-graph) and eventually converging to the concentration graph when the number of conditioning variables equals the separability order.

In summary, concentration graphs represent full conditional independence given all other variables, while covariance graphs represent marginal independence between variables. The separation criteria for both types of graphs are similar, but with a key difference in how they handle k > 0.
