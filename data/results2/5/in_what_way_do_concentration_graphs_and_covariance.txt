=== PROMPT ===
You are a friendly educator.
Provide a highly readable and intuitive explanation, suitable for non-experts. It’s okay to simplify complex concepts and use your own words. Use external facts when necessary, but avoid overloading with citations.

Query:
In what way do concentration graphs and covariance graphs differ in representing conditional independence, and how is this distinction formalized in terms of separation criteria?

Context:
Source: 0705.1613.md (chunk 5)
Content: the associated graphical model is called a concentration graph model (see Lauritzen
(1996)). Dempster (1972) studied concentration graph models for Gaussian distributions under the name of covariance selection or covariance selection models. The absence of an edge between a given pair of vertices (α,β) in the associated graph indicates that the random variable Xα is independent of Xβ given all the other variables
X−αβ = (Xγ,γ ̸= α,β)[′]. These models are very well studied, especially the ones corresponding to Gaussian distributions (see Whittaker (1990), Lauritzen (1996), Edwards
(2000) and, recently, Letac and Massam (2007) and Rajaratnam et al. (2008)). The separation criteria defined on such graphs is a simple separation criteria on undirected graphs:
S ⊆ V separates two disjoint non-empty subsets A and B of V if any path joining a vertex
in A and another in B intersects S.
Other graphical models are represented by graphs with bi-directed edges. These models

Source: 0705.1613.md (chunk 9)
Content: (α,β) /∈ E ⇐⇒ Xα ⊥⊥ Xβ | X−αβ,

and if A, B and S are three disjoint non-empty subsets of V

S separates A and B in G ⇐⇒ XA ⊥⊥ XB|XS.

The aim of this paper is to prove the existence of a relationship between the cardinality
of the separators in G and the maximum number of conditioning variables needed to
determine the full conditional independencies in P . We proceed first by defining a new
parameter for undirected graphs, called the “separability order ”. Subsequently we prove
that when we condition on a fixed number of variables equal to this separability order,
the graph that is obtained is exactly the concentration graph.
The paper is organized as follows. Section 2 is devoted to the definition of this parameter and of some properties thereof. In Section 3, we will define a sequence of undirected graphs constructed due to conditional independencies for a given fixed number
k ∈{0,..., |V | − 2}. More precisely, we define the k-graph Gk = (V,Ek) by

Source: 0705.1613.md (chunk 1)
Content: A concentration graph associated with a random vector is an undirected graph where each
vertex corresponds to one random variable in the vector. The absence of an edge between any
pair of vertices (or variables) is equivalent to full conditional independence between these two
variables given all the other variables. In the multivariate Gaussian case, the absence of an edge
corresponds to a zero coefficient in the precision matrix, which is the inverse of the covariance
matrix.
It is well known that this concentration graph represents some of the conditional independencies in the distribution of the associated random vector. These conditional independencies
correspond to the “separations” or absence of edges in that graph. In this paper we assume that
there are no other independencies present in the probability distribution than those represented
by the graph. This property is called the perfect Markovianity of the probability distribution

Source: 0705.1613.md (chunk 30)
Content: concentration graph model G is equal to the k-partial graph G[p]k [as defined in (][6][) when]
k is smaller than the separability order of G, referred to in that paper as the “outer
connectivity of the missing edges”. The result in Theorem 4, however, is based on a
construction of a sequence of nested graphs. It starts from the covariance graph, that is,
G0, the 0-graph, and it becomes stationary and equal to the concentration graph when
the number of conditioning variables is equal to the separability order of the concentration
graph.
In Lemma 8, we show that k-partial graphs and k-graphs are equal when the perfect
Markovianity assumption is satisfied. Next, we give a corollary of Theorem 4. In Corollary 9 we give a condition that allows us to determine the last undirected k-graph in
the sequence of nested graphs obtained due to Theorem 4. This condition is given in
term of the degree two of the k-graphs, not as in Castello and Roverato (2006), where

Source: 0705.1613.md (chunk 10)
Content: (α,β) /∈ Ek ⇐⇒ ∃S ⊆ V \ {α,β} such that |S| = k and Xα ⊥⊥ Xβ|XS. (5)

When k = 0, the conditional independence given an empty set corresponds to the
marginal independence between Xα and Xβ. In the case when k = 0, the corresponding k-graph is then denoted by G0 and constructed using the pairwise Markov property
with respect to bi-directed graphs (see Cox and Wermuth (1996) and Chaudhuri et al.
(2007)). We mean that

α ̸∼G0 β ⇐⇒ Xα ⊥⊥ Xβ.

The graph G0 is also called a covariance graph (see Chaudhuri et al. (2007)). Wille and
B¨uhlman (2006) define a graph called a 0–1 graph, which corresponds to a graph with a
set of edges equal to E0 ∩ E1. We will show later (see Lemma 7) that this graph is equal
to G1. Castello and Roverato (2006) consider so called k-partial graphs G[p]k [= (][V,E]k[p][),]
which are defined as follows:

α ̸∼Gk β ⇐⇒ ∃S ⊆ V \ {α,β} such that |S| ≤ k and Xα ⊥⊥ Xβ|XS. (6)

=== ANSWER ===
I'd be happy to help explain the difference between concentration graphs and covariance graphs in a way that's easy to understand.

**What are concentration graphs and covariance graphs?**

Imagine you have a bunch of random variables, like X1, X2, X3, etc., that are all connected to each other. A concentration graph is an undirected graph where each vertex represents one of these random variables. If there's no edge between two vertices (Xα and Xβ), it means that Xα is independent of Xβ given all the other variables.

A covariance graph is similar, but it's a bit more complicated. It's also an undirected graph, but instead of just looking at independence, it looks at how much each variable is related to every other variable. In particular, if there's no edge between two vertices (Xα and Xβ), it means that Xα is independent of Xβ given all the other variables.

**How do concentration graphs and covariance graphs differ?**

The key difference between concentration graphs and covariance graphs is what they represent. Concentration graphs only look at independence, while covariance graphs look at how much each variable is related to every other variable.

Think of it like this: if you have two friends, Alice and Bob, who are both connected to a third friend, Charlie. In a concentration graph, the absence of an edge between Alice and Bob would mean that they're independent of each other given Charlie. But in a covariance graph, the absence of an edge would mean that Alice and Bob are not related to each other at all - not even through Charlie.

**Separation criteria**

In both cases, if you have two disjoint subsets of vertices (A and B), a separator is a set of vertices S such that any path joining a vertex in A to a vertex in B intersects S. This means that the variables in A are independent of the variables in B given all the other variables.

The key difference is what kind of separation criteria are used. In concentration graphs, the separation criteria is simply whether there's an edge between two vertices or not. In covariance graphs, the separation criteria involves looking at how much each variable is related to every other variable.

**Formal distinction**

In formal terms, a concentration graph represents conditional independence in the sense that if there's no edge between Xα and Xβ, then Xα ⊥⊥ Xβ | X−αβ. In contrast, a covariance graph represents conditional independence in the sense that if there's no edge between Xα and Xβ, then Xα ⊥⊥ Xβ | X−αβ, X−αβ ≠ ∅.

I hope this helps clarify the difference between concentration graphs and covariance graphs! Let me know if you have any further questions.
