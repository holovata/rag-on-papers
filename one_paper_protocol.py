#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PDFAnalysisPipeline Ñ OpenAI Ğ¸ ÑÑ‚Ñ€Ğ¸Ğ¼Ğ¸Ğ½Ğ³Ğ¾Ğ²Ñ‹Ğ¼ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¾Ğ¼, Ğ³Ğ´Ğµ Ñ‚ĞµĞºÑÑ‚Ñ‹ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ñ…Ñ€Ğ°Ğ½ÑÑ‚ÑÑ Ğ² YAML.

Ğ¨Ğ°Ğ³Ğ¸:
 1) Ğ‘ĞµÑ€Ñ‘Ñ‚ Ğ¾Ñ‡Ğ¸Ñ‰ĞµĞ½Ğ½Ñ‹Ğ¹ Markdown Ğ¸Ğ· GridFS (file_buffer_db â†’ filename_clean.md).
 2) ĞŸĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ingest_from_gridfs Ğ¸Ğ· papers_vectorization_MONGO
    (Ğ¾Ñ‡Ğ¸Ñ‰Ğ°ĞµÑ‚ paper_chunks, Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ MD Ğ½Ğ° Ñ‡Ğ°Ğ½ĞºĞ¸ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ OpenAI).
 3) ĞŸĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ build_vector_index Ğ¸Ğ· papers_vectorization_MONGO Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ/Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°.
 4) Ğ”Ğ»Ñ Ñ€ĞµÑ‚Ñ€Ğ¸Ğ²Ğ°Ğ»Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ get_relevant_chunks Ğ¸Ğ· papers_retrieval_MONGO.
 5) Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑÑ‚Ğ°Ğ¿Ñ‹ (retrieval_prompt, reasoning, qa_over_pdf) Ñ‡Ğ¸Ñ‚Ğ°ÑÑ‚ â€œactionâ€ Ğ¸Ğ· YAML
    Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ ChatOpenAI.stream Ğ´Ğ»Ñ ÑÑ‚Ñ€Ğ¸Ğ¼Ğ¸Ğ½Ğ³Ğ° Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°.
 6) Ğ’ÑĞµ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ ÑˆĞ°Ğ³Ğ¸ reasoning Ğ²Ñ‹Ğ²Ğ¾Ğ´ÑÑ‚ÑÑ Ğ½Ğ° ÑĞºÑ€Ğ°Ğ½.
"""

import os
import sys
import tempfile
from pathlib import Path

import yaml
import gridfs
from pymongo.mongo_client import MongoClient
from pymongo.server_api import ServerApi

from langchain_openai import ChatOpenAI

# ĞŸĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸
from src.retrieval.papers_retrieval_MONGO import get_relevant_chunks
from src.data_processing.papers_vectorization_MONGO import clear_all_before_ingest, mongo_client

from dotenv import load_dotenv

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ĞšĞ¾Ğ½ÑÑ‚Ğ°Ğ½Ñ‚Ñ‹ / ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

load_dotenv()
MONGODB_URI = os.getenv("MONGODB_URI")
if not MONGODB_URI:
    print("âœ˜ MONGODB_URI not set in environment (.env)", file=sys.stderr)
    sys.exit(1)

ARXIV_DB_NAME    = "arxiv_db"
CHUNKS_COLL_NAME = "paper_chunks"
GRIDFS_DB_NAME   = "file_buffer_db"

# Ğ˜Ğ¼Ñ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ğ´ĞµĞºÑĞ° (ÑĞ¾Ğ²Ğ¿Ğ°Ğ´Ğ°ĞµÑ‚ Ñ CHNK_INDEX Ğ¸Ğ· papers_vectorization_MONGO)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ğ’ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ: Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ¾Ñ‡Ğ¸Ñ‰ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ MD Ğ¸Ğ· GridFS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def fetch_clean_markdown_from_gridfs(pdf_path: str) -> str:
    """
    Ğ˜Ñ‰ĞµÑ‚ Ğ² GridFS (file_buffer_db) Ñ„Ğ°Ğ¹Ğ» "<stem>_clean.md", ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾
    Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ¿Ğ°Ğ¿ĞºÑƒ Ğ¸ Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ¿ÑƒÑ‚ÑŒ. Ğ•ÑĞ»Ğ¸ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½, Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ "".
    """
    stem = Path(pdf_path).stem
    clean_filename = f"{stem}_clean.md"

    client = MongoClient(MONGODB_URI, server_api=ServerApi("1"))
    db = client[GRIDFS_DB_NAME]
    fs = gridfs.GridFS(db)

    grid_out = fs.find_one({"filename": clean_filename})
    if not grid_out:
        print(f"[fetch_clean_markdown] âœ˜ '{clean_filename}' not found in GridFS.", file=sys.stderr)
        client.close()
        return ""

    content_bytes = grid_out.read()
    client.close()

    tmp_dir = tempfile.mkdtemp(prefix="clean_md_")
    tmp_path = Path(tmp_dir) / clean_filename
    try:
        tmp_path.write_bytes(content_bytes)
    except Exception as e:
        print(f"[fetch_clean_markdown] âœ˜ Cannot write to {tmp_path}: {e}", file=sys.stderr)
        return ""
    return str(tmp_path)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# PDFAnalysisPipeline
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class PDFAnalysisPipeline:
    def __init__(self, protocol_path: str):
        self.protocol = self._load_protocol(protocol_path)
        self.context = {}
        self.llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.4)

    def _load_protocol(self, path: str) -> dict:
        with open(path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)

    def _resolve_parameters(self, params: dict) -> dict:
        """
        ĞŸĞ¾Ğ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ· self.context Ğ´Ğ»Ñ ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ğ¾Ğ² Ğ²Ğ¸Ğ´Ğ° "{{var}}".
        """
        resolved = {}
        for k, v in params.items():
            if isinstance(v, str) and v.startswith("{{") and v.endswith("}}"):
                key = v[2:-2].strip()
                resolved[k] = self.context.get(key, "")
            else:
                resolved[k] = v
        return resolved

    def _stream_llm(self, prompt: str) -> str:
        """
        Ğ¡Ñ‚Ñ€Ğ¸Ğ¼Ğ¸Ğ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚ LLM, Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ°ĞµĞ¼ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾Ñ‡Ğ½Ğ¾ Ğ¸ Ğ½Ğ°ĞºĞ°Ğ¿Ğ»Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ² Ğ¾Ğ´Ğ½Ñƒ ÑÑ‚Ñ€Ğ¾ĞºÑƒ.
        """
        streamed = ""
        for chunk in self.llm.stream([("user", prompt)]):
            print(chunk.content, end="", flush=True)
            streamed += chunk.content
        print()
        return streamed

    def run_pipeline_with_progress(self, user_query: str, user_pdf: str, progress_callback=None):
        """
        Ğ˜Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ ÑˆĞ°Ğ³Ğ°Ğ¼ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ»Ğ°: Ğ¾Ñ‚Ğ´Ğ°Ñ‘Ñ‚ (stage, status, context).
        Ğ•ÑĞ»Ğ¸ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ğ½ progress_callback, ĞµĞ³Ğ¾ Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ² ÑˆĞ°Ğ³Ğµ reasoning Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ CoT-Ğ¿Ğ¾Ğ´ÑˆĞ°Ğ³Ğ°.
        """
        self.context.update({"user_query": user_query, "user_pdf": user_pdf})

        for step in self.protocol["pipeline"]:
            stage  = step["stage"]
            action = step.get("action", "").strip()
            print(f"\n>> [{stage}]")  # Ğ—Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğº ÑÑ‚Ğ°Ğ¿Ğ°
            yield stage, "in_progress", dict(self.context)

            params = self._resolve_parameters(step.get("parameters", {}))
            self.context.update(params)

            if stage == "retrieval":
                # Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼ prompt Ğ¸Ğ· action: Ğ¿Ğ¾Ğ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹
                # prompt = action.format(**self.context)
                # print(f"-- Prompt for retrieval:\n{prompt}\n")
                # Ğ—Ğ´ĞµÑÑŒ Ğ½Ğµ ÑÑ‚Ñ€Ğ¸Ğ¼Ğ¸Ğ¼, Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¸Ğ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ Ğ¸ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ñ‡Ğ°Ğ½ĞºĞ¸
                docs = get_relevant_chunks(self.context["user_query"], top_n=params.get("top_n", 3))
                self.context["retrieval_output"] = docs
                print(f"-- Retrieved {len(docs)} chunks.")

            elif stage == "reasoning":
                # Ğ‘ĞµÑ€Ñ‘Ğ¼ ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½ prompt Ğ¸Ğ· action
                # base_prompt = action.format(**self.context)
                # print(f"-- Prompt for reasoning:\n{base_prompt}\n")

                # Ğ Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ½Ğ° Ğ¿Ğ¾Ğ´ÑˆĞ°Ğ³Ğ° CoT: Ğ·Ğ´ĞµÑÑŒ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ĞµĞ¼, Ñ‡Ñ‚Ğ¾ action Ğ´Ğ°Ñ‘Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¹ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ
                # ĞœÑ‹ Ğ²ÑÑ‘ ĞµÑ‰Ñ‘ Ğ´ĞµĞ»Ğ°ĞµĞ¼ multi-step CoT, Ğ½Ğ¾ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ÑˆĞ°Ğ³ â€“ ÑÑ‚Ğ¾ base_prompt
                # Ğ¨Ğ°Ğ³ 0: initial answer
                # initial_answer = self._stream_llm(step.get("prompt_template", "").strip())
                reasoning_steps = []
                # all_answers = [initial_answer]
                # if progress_callback:
                #    progress_callback(0, "Initial Answer", initial_answer)

                # â”€â”€â”€â”€â”€ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ run_pipeline_with_progress â”€â”€â”€â”€â”€
                for step in self.protocol["pipeline"]:
                    stage = step["stage"]
                    prompt = step.get("prompt_template", "").strip()  # Ğ²Ğ¼ĞµÑÑ‚Ğ¾ action
                    cot_qs = step.get("cot_questions")  # ÑĞ¿Ğ¸ÑĞ¾Ğº Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ
                    ...
                    # â€” retrieval â€”
                    '''if stage == "retrieval":
                        print(prompt.format(**self.context), "\n")
                        docs = get_relevant_chunks(...)'''

                    # â€” reasoning â€”
                    if stage == "reasoning":
                        base_prompt = prompt.format(**self.context)
                        print("-- Prompt for reasoning:\n", base_prompt, "\n")

                        initial_answer = self._stream_llm(base_prompt)
                        reasoning_steps, all_answers = [], [initial_answer]

                        questions = cot_qs or [  # ĞµÑĞ»Ğ¸ Ğ² YAML Ğ½ĞµÑ‚ ÑĞ¿Ğ¸ÑĞºĞ° â€“ Ğ´ĞµÑ„Ğ¾Ğ»Ñ‚
                            "Identify potential weaknesses or gaps in your answer.",
                            "How can you address those gaps?",
                            "What extra details would improve your answer?",
                            "Does your answer fully align with the query?",
                            "How confident are you, and what would increase confidence?",
                            "Summarize an improved final answer."
                        ]

                        context_chunks = "\n".join(
                            f"Chunk {i + 1}: {ch.page_content}"
                            for i, ch in enumerate(self.context.get("retrieval_output", [])[:5])
                        )

                        for i, question in enumerate(questions, 1):
                            prev = "\n".join(f"Answer {j + 1}: {ans}"
                                             for j, ans in enumerate(all_answers))
                            iteration_prompt = f"""Iteration {i}:
                Query: {self.context['user_query']}

                Context:
                {context_chunks}

                Previous Answers:
                {prev}

                Refinement Question: {question}

                Please refine your response."""
                            analysis = self._stream_llm(iteration_prompt)
                            reasoning_steps.append({"step": question,
                                                    "analysis": analysis,
                                                    "iteration": i})
                            all_answers.append(analysis)
                            if progress_callback:
                                progress_callback(i, question, analysis)

                final_answer = all_answers[-1]
                if progress_callback:
                    progress_callback(0, "CoT Complete", final_answer)

                self.context.update({
                    "initial_answer":   initial_answer,
                    "reasoning_steps":  reasoning_steps,
                    "final_answer":     final_answer
                })

            elif stage == "qa_over_pdf":
                prompt = action.format(**self.context)
                print(f"-- Prompt for QA:\n{prompt}\n")
                answer = self._stream_llm(prompt)
                self.context["qa_over_pdf_output"] = answer

            else:
                func = getattr(self, step["function"], None)
                if func:
                    output = func(**params)
                    self.context[f"{step['function']}_output"] = output
                else:
                    print(f"[ERROR] Function '{step['function']}' not found", file=sys.stderr)
                    yield stage, "error", dict(self.context)
                    continue

            yield stage, "done", dict(self.context)

        yield "pipeline_complete", "", dict(self.context)

    def run_pipeline(self, *args, **kwargs) -> dict:
        for _ in self.run_pipeline_with_progress(*args, **kwargs):
            pass
        return self.context


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ğ¢Ğ¾Ñ‡ĞºĞ° Ğ²Ñ…Ğ¾Ğ´Ğ°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

if __name__ == "__main__":
    protocol_file = "one_paper_protocol.yaml"
    pipeline = PDFAnalysisPipeline(protocol_file)

    FILE_BUFFER_DB = "file_buffer_db"  # Ğ‘Ğ”-Ğ±ÑƒÑ„ĞµÑ€ Ğ´Ğ»Ñ GridFS
    client = mongo_client()
    db_buffer  = client[FILE_BUFFER_DB]

    clear_all_before_ingest(db_buffer)
    user_pdf = input("Enter path to PDF: ").strip()
    user_query = input("Enter your query: ").strip()

    def progress_callback(step_num, question, analysis):
        print(f"\n--- CoT Step {step_num}: {question} ---")
        print(analysis)
        print("----------------------------\n")

    for stage, status, ctx in pipeline.run_pipeline_with_progress(user_query, user_pdf, progress_callback):
        print(f"[{stage}] {status}")

    final = pipeline.context.get("final_answer")
    if final:
        print("\nğŸ¯ Final Answer:")
        print(final)
