# streamlit: name = üìö Multi-PDF Mode

import streamlit as st
import os
import io
import contextlib

from pathlib import Path

from src.data_processing.papers_vectorization_MONGO import (
    ingest_pdfs_to_markdowns,
    ingest_multi_chunks,
    build_vector_index,
    clear_all_before_ingest,
)
from src.retrieval.papers_retrieval_MONGO import get_relevant_chunks
from n_papers_protocol import MultiPDFAnalysisPipeline

from my_utils import save_uploaded_file, check_chroma_connection, clear_folder

def init_session():
    page_id = "MultiPDF"
    session_defaults = {
        f"uploaded_files_{page_id}": [],
        f"query_{page_id}": "",
        f"pipeline_instance_{page_id}": None,
        f"protocol_output_{page_id}": ""
    }
    for key, value in session_defaults.items():
        if key not in st.session_state:
            st.session_state[key] = value

def update_stage_indicator(
    stage, status, data,
    container, indicators, labels,
    cot_container=None, cot_indicators=None
):
    """–û–±–Ω–æ–≤–ª—è–µ—Ç —Å—Ç–∞—Ç—É—Å –∫–∞–∂–¥–æ–π —Å—Ç–∞–¥–∏–∏, –≤–∫–ª—é—á–∞—è CoT-–ø–æ–¥—à–∞–≥–∏."""
    stage_label = labels.get(stage, stage)
    if stage not in indicators:
        indicators[stage] = container.empty()
        indicators[stage].markdown(f"‚è≥ **{stage_label}**: Waiting...")
    if status == "in_progress":
        if stage == "reasoning" and isinstance(data, dict) and "reasoning_output" in data:
            # –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è reasoning, –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π —à–∞–≥ CoT
            reasoning_out = data["reasoning_output"]
            for step_info in reasoning_out.get("reasoning_steps", []):
                step_label = step_info["step"]
                analysis = step_info["analysis"]
                if step_label not in cot_indicators and cot_container is not None:
                    cot_indicators[step_label] = cot_container.expander(f"üìù {step_label}")
                    cot_indicators[step_label].markdown(f"{analysis}")
            indicators[stage].markdown(f"üöÄ **{stage_label}**: In progress...")
        else:
            indicators[stage].markdown(f"üöÄ **{stage_label}**: In progress...")
    elif status == "done":
        indicators[stage].markdown(f"‚úÖ **{stage_label}**: Completed")
    elif status == "error":
        indicators[stage].markdown(f"‚ùå **{stage_label}**: Error")

def main():
    page_id = "MultiPDF"
    init_session()
    st.set_page_config(page_title="Multi-PDF Analysis", layout="wide")
    st.title("üìö Multi-PDF Document Analysis")

    # Sidebar: –∑–∞–≥—Ä—É–∑–∫–∞ –∏ –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–π
    with st.sidebar:
        st.header("‚öôÔ∏è Document Settings")
        uploaded_files = st.file_uploader(
            "Upload PDF files",
            type=["pdf"],
            help="Upload one or more PDFs for analysis",
            accept_multiple_files=True
        )
        st.session_state[f"uploaded_files_{page_id}"] = uploaded_files

        persist_dir = r"C:\Work\diplom2\rag_on_papers\data\n_papers\embeddings"
        connection_status = check_chroma_connection(persist_dir)
        st.markdown(f"**Embedding Database Status:** {connection_status}")

    # –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—É—Ç–∏
    target_folder = r"C:\Work\diplom2\rag_on_papers\data\n_papers"
    embeddings_folder = os.path.join(target_folder, "embeddings")

    st.subheader("Step 1: Document Processing")
    st.markdown(
        "Click the button below to prepare your PDF files: "
        "convert them to Markdown, remove tables, save them to GridFS, "
        "split into chunks, and build the vector index."
    )

    if st.button("Process Documents"):
        if not st.session_state[f"uploaded_files_{page_id}"]:
            st.warning("Please upload at least one PDF file before processing.")
        else:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã –≤ –ø–∞–ø–∫—É target_folder
            for uploaded_file in st.session_state[f"uploaded_files_{page_id}"]:
                file_path = os.path.join(target_folder, uploaded_file.name)
                if not save_uploaded_file(uploaded_file, file_path):
                    st.error(f"Failed to save {uploaded_file.name}.")
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ø–∏—Å–æ–∫ –∏–º—ë–Ω, —á—Ç–æ–±—ã –Ω–µ —É–¥–∞–ª—è—Ç—å –∏—Ö –ø—Ä–∏ –æ—á–∏—Å—Ç–∫–µ
            preserve = ",".join([f.name for f in st.session_state[f"uploaded_files_{page_id}"]])
            clear_folder(target_folder, preserve_files=preserve)
            # –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–µ embeddings
            clear_folder(embeddings_folder)

            # –ó–∞–ø—É—Å–∫–∞–µ–º ingestion + indexing —á–µ—Ä–µ–∑ MultiPDFAnalysisPipeline
            processor = MultiPDFAnalysisPipeline("n_papers_protocol.yaml")
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä pipeline –≤ session_state, —á—Ç–æ–±—ã –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –µ–≥–æ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            st.session_state[f"pipeline_instance_{page_id}"] = processor

            with st.spinner("Processing documents..."):
                ingest_result = processor.run_ingest_and_index(pdf_folder=target_folder)
            st.success(f"Document processing completed! Inserted {ingest_result.get('multi_chunk_count', 0)} chunks.")

    if st.session_state.get(f"pipeline_instance_{page_id}") is None:
        st.subheader("Step 2: Document Analysis")
        st.info("Please complete Step 1 to process the PDFs before querying.")
        return

    st.subheader("Step 2: Document Analysis")
    st.markdown("Enter a query to analyze the documents (after you have clicked ‚ÄúProcess Documents‚Äù).")
    st.session_state[f"query_{page_id}"] = st.text_input(
        "Enter your query:", key="multi_pdf_query"
    )

    if st.button("Analyze Documents"):
        if not st.session_state[f"query_{page_id}"].strip():
            st.warning("Please enter a query.")
        else:
            processor = st.session_state.get(f"pipeline_instance_{page_id}")
            if processor is None:
                st.error("–°–Ω–∞—á–∞–ª–∞ –Ω–∞–∂–º–∏—Ç–µ ¬´Process Documents¬ª, —á—Ç–æ–±—ã –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å PDF.")
            else:
                # –ö–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã –¥–ª—è —Å—Ç–∞—Ç—É—Å–æ–≤ —Å—Ç–∞–¥–∏–π –∏ CoT
                stage_container = st.container()
                cot_container = st.container()
                stage_indicators = {}
                cot_indicators = {}
                STAGE_LABELS = {
                    "ingest_and_index": "Ingest & Index",
                    "retrieval": "Retrieving Relevant Chunks",
                    "reasoning": "Chain-of-Thought Analysis",
                    "qa_over_pdf": "Generating Final Answer",
                    "synthesis": "Synthesis",
                    "pipeline_complete": "Pipeline Complete"
                }

                # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ stdout
                log_stream = io.StringIO()
                try:
                    with st.spinner("Analyzing documents..."):
                        with contextlib.redirect_stdout(log_stream):
                            events = processor.run_pipeline_with_progress(
                                user_query=st.session_state[f"query_{page_id}"],
                                pdf_folder=target_folder,
                                progress_callback=lambda stage, status, data: update_stage_indicator(
                                    stage, status, data,
                                    stage_container, stage_indicators, STAGE_LABELS,
                                    cot_container, cot_indicators
                                )
                            )
                            # –ü–µ—Ä–µ–±–∏—Ä–∞–µ–º –≤—Å–µ —Å–æ–±—ã—Ç–∏—è, —á—Ç–æ–±—ã pipeline –ø–æ–ª–Ω–æ—Å—Ç—å—é –∑–∞–≤–µ—Ä—à–∏–ª—Å—è
                            for _ in events:
                                pass

                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª–æ–≥–∏ –≤ session_state
                    st.session_state[f"protocol_output_{page_id}"] = log_stream.getvalue()
                    st.divider()

                    # –í—ã–≤–æ–¥–∏–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç reasoning –∏ QA
                    result = processor.context

                    # –ï—Å–ª–∏ –±—ã–ª–∏ CoT-—à–∞–≥–∏, –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –∏—Ö –ø–æ–¥ —ç–∫—Å–ø–∞–Ω–¥–µ—Ä–∞–º–∏
                    reasoning_out = result.get("reasoning_output", {})
                    if reasoning_out:
                        for step_info in reasoning_out.get("reasoning_steps", []):
                            with st.expander(f"üìù {step_info['step']}"):
                                st.markdown(step_info["analysis"])

                    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏—Ç–æ–≥–æ–≤—ã–π –æ—Ç–≤–µ—Ç
                    st.subheader("üéØ Final Answer")
                    final_synthesis = result.get("synthesis_output", "")
                    if final_synthesis:
                        st.markdown(final_synthesis)
                    else:
                        qa_out = result.get("qa_over_pdf_output", "")
                        if qa_out:
                            st.markdown(qa_out)
                        else:
                            st.info("No answer produced.")

                except Exception as e:
                    st.error(f"üö® Document analysis failed: {str(e)}")
                finally:
                    st.divider()
                    with st.expander("üìú Execution Logs"):
                        logs = st.session_state.get(f"protocol_output_{page_id}", "")
                        if logs.strip():
                            st.code(logs, language="log")
                        else:
                            st.info("No logs available.")

if __name__ == "__main__":
    main()
